INFO:	main:	Namespace(n_degree=20, n_head=2, n_layer=2, drop_out=0.1, reg=0.1, node_dim=32, time_dim=32, agg_method='attn', attn_mode='map', uniform=True, new_node=False, time='disentangle', disencomponents=10, data='ml-100k', samplerate=1.0, popnegsample=False, timepopnegsample=False, negsampleeval=3000, bs=256, n_epoch=500, lr=0.0001, prefix='ml100k', gpu=0, seed=42, l2=0.0001)
INFO:	TGSRec:	Aggregation uses attention model
INFO:	attention_model:	Using map based attention
INFO:	attention_model:	Using scaled prod attention
INFO:	attention_model:	Using map based attention
INFO:	attention_model:	Using scaled prod attention
INFO:	TGSRec:	Using disentangle time encoding
INFO:	trainer:	Model built successfully
INFO:	trainer:	TGRec(
  (node_hist_embed): Embedding(2626, 32)
  (merge_layer): MergeLayer(
    (fc1): Linear(in_features=64, out_features=32, bias=True)
    (fc2): Linear(in_features=32, out_features=32, bias=True)
    (act): ReLU()
  )
  (attn_model_list): ModuleList(
    (0-1): 2 x CrossAttentionModel(
      (ffn): MergeLayer(
        (fc1): Linear(in_features=96, out_features=32, bias=True)
        (fc2): Linear(in_features=32, out_features=32, bias=True)
        (act): ReLU()
      )
      (multi_head_target): MapBasedMultiHeadAttention(
        (wq_node_transform): Linear(in_features=64, out_features=64, bias=False)
        (wk_node_transform): Linear(in_features=64, out_features=64, bias=False)
        (wv_node_transform): Linear(in_features=64, out_features=64, bias=False)
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (fc): Linear(in_features=64, out_features=64, bias=True)
        (act): LeakyReLU(negative_slope=0.2)
        (weight_map): Linear(in_features=64, out_features=1, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=2)
      )
      (self_attn_model): SelfAttentionModel(
        (ffn): MergeSelfAttnLayer(
          (fc1): Linear(in_features=96, out_features=5120, bias=True)
          (fc2): Linear(in_features=5120, out_features=64, bias=True)
          (act): ReLU()
        )
        (multi_head_target): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (fc): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (time_encoder): DisentangleTimeEncode()
  (affinity_score): MergeLayer(
    (fc1): Linear(in_features=64, out_features=32, bias=True)
    (fc2): Linear(in_features=32, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:	trainer:	Optimizer: Adam with lr: 0.0001 and l2: 0.0001
INFO:	trainer:	Learning rate scheduler: LinearLR
INFO:	trainer:	Warmup scheduler: LinearWarmup
INFO:	trainer:	Device: cuda:0
INFO:	trainer:	Number of nodes: 2625
INFO:	trainer:	Number of edges: 100000
INFO:	callbacks:	Early stopping monitor: max_round=10, higher_better=True, tolerance=0.001
INFO:	main:	Commencing training for 500 epochs
INFO:	trainer:	Epoch [1/500]:	Train Loss: 16.9108	Train Acc: 0.6651	Train AP: 0.6806	Train F1: 0.7232	Train AUC: 0.7268	Val Acc: 0.5526	Val AP: 0.6864	Val F1: 0.6908	Val AUC: 0.7338	Test Acc: 0.5504	Test AP: 0.7070	Test F1: 0.6893	Test AUC: 0.7613
