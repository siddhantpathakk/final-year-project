INFO:	main:	Namespace(n_degree=30, n_head=2, n_layer=2, drop_out=0.3, reg=0.3, node_dim=32, time_dim=32, agg_method='attn', attn_mode='prod', uniform=True, new_node=False, time='disentangle', disencomponents=10, data='ml-100k', train_test_val='80-10-10', samplerate=1.0, popnegsample=False, timepopnegsample=False, negsampleeval=1000, bs=128, n_epoch=200, lr=0.001, prefix='ml100k', gpu=0, seed=42, l2=0.0001, ckpt_epoch=10)
INFO:	data_utils:	Train split: 80%, Val split: 10%, Test split: 10%
INFO:	TGSRec:	Aggregation uses attention model
INFO:	TGSRec:	Using disentangle time encoding
INFO:	trainer_utils:	Model built successfully
INFO:	trainer_utils:	TGRec(
  (node_hist_embed): Embedding(2626, 32)
  (merge_layer): MergeLayer(
    (fc1): Linear(in_features=64, out_features=32, bias=True)
    (fc2): Linear(in_features=32, out_features=32, bias=True)
    (act): ReLU()
  )
  (attn_model_list): ModuleList(
    (0-1): 2 x CrossAttentionModel(
      (ffn): MergeLayer(
        (fc1): Linear(in_features=96, out_features=32, bias=True)
        (fc2): Linear(in_features=32, out_features=32, bias=True)
        (act): ReLU()
      )
      (multi_head_target): MultiHeadAttention(
        (w_qs): Linear(in_features=64, out_features=64, bias=False)
        (w_ks): Linear(in_features=64, out_features=64, bias=False)
        (w_vs): Linear(in_features=64, out_features=64, bias=False)
        (attention): ScaledDotProductAttention(
          (dropout): Dropout(p=0.3, inplace=False)
          (softmax): Softmax(dim=2)
        )
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (fc): Linear(in_features=64, out_features=64, bias=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (self_attn_model): SelfAttentionModel(
        (ffn): MergeSelfAttnLayer(
          (fc1): Linear(in_features=96, out_features=3840, bias=True)
          (fc2): Linear(in_features=3840, out_features=64, bias=True)
          (act): ReLU()
        )
        (multi_head_target): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (fc): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (time_encoder): DisentangleTimeEncode()
  (affinity_score): MergeLayer(
    (fc1): Linear(in_features=64, out_features=32, bias=True)
    (fc2): Linear(in_features=32, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:	trainer_utils:	Number of parameters: 1402177 (trainable: 1402177)
INFO:	trainer_utils:	Loss function: BPR + L2 regularization
INFO:	trainer_utils:	Optimizer: Adam with lr: 0.001 and l2: 0.0001
INFO:	trainer_utils:	Learning rate scheduler: LinearLR
INFO:	trainer_utils:	Warmup scheduler: LinearWarmup with warmup period: 15
INFO:	trainer_utils:	Device: cuda:0
INFO:	trainer_utils:	Number of nodes: 2625
INFO:	trainer_utils:	Number of edges: 100000
INFO:	main:	Commencing training for 200 epochs
INFO:	trainer:	Epoch [1/200]	[Training]:	Loss: 50.292	Acc: 0.653	AP: 0.594	F1: 0.723	AUC: 0.653
