INFO:	main:	Namespace(n_degree=30, n_head=2, n_layer=2, drop_out=0.3, reg=0.3, node_dim=32, time_dim=32, agg_method='attn', attn_mode='prod', uniform=True, new_node=False, time='disentangle', disencomponents=10, data='ml-100k', train_test_val='80-10-10', samplerate=1.0, popnegsample=False, timepopnegsample=False, negsampleeval=1000, bs=128, n_epoch=200, lr=0.001, prefix='ml100k', gpu=0, seed=42, l2=0.0001, ckpt_epoch=10)
INFO:	data_utils:	Train split: 80%, Val split: 10%, Test split: 10%
INFO:	TGSRec:	Aggregation uses attention model
INFO:	TGSRec:	Using disentangle time encoding
INFO:	trainer_utils:	Model built successfully
INFO:	trainer_utils:	TGRec(
  (node_hist_embed): Embedding(2626, 32)
  (merge_layer): MergeLayer(
    (fc1): Linear(in_features=64, out_features=32, bias=True)
    (fc2): Linear(in_features=32, out_features=32, bias=True)
    (act): ReLU()
  )
  (attn_model_list): ModuleList(
    (0-1): 2 x CrossAttentionModel(
      (ffn): MergeLayer(
        (fc1): Linear(in_features=96, out_features=32, bias=True)
        (fc2): Linear(in_features=32, out_features=32, bias=True)
        (act): ReLU()
      )
      (multi_head_target): MultiHeadAttention(
        (w_qs): Linear(in_features=64, out_features=64, bias=False)
        (w_ks): Linear(in_features=64, out_features=64, bias=False)
        (w_vs): Linear(in_features=64, out_features=64, bias=False)
        (attention): ScaledDotProductAttention(
          (dropout): Dropout(p=0.3, inplace=False)
          (softmax): Softmax(dim=2)
        )
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (fc): Linear(in_features=64, out_features=64, bias=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
      (self_attn_model): SelfAttentionModel(
        (ffn): MergeSelfAttnLayer(
          (fc1): Linear(in_features=96, out_features=3840, bias=True)
          (fc2): Linear(in_features=3840, out_features=64, bias=True)
          (act): ReLU()
        )
        (multi_head_target): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=64, bias=False)
          (w_ks): Linear(in_features=64, out_features=64, bias=False)
          (w_vs): Linear(in_features=64, out_features=64, bias=False)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (fc): Linear(in_features=64, out_features=64, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (time_encoder): DisentangleTimeEncode()
  (affinity_score): MergeLayer(
    (fc1): Linear(in_features=64, out_features=32, bias=True)
    (fc2): Linear(in_features=32, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:	trainer_utils:	Number of parameters: 1402177 (trainable: 1402177)
INFO:	trainer_utils:	Loss function: BPR + L2 regularization
INFO:	trainer_utils:	Optimizer: Adam with lr: 0.001 and l2: 0.0001
INFO:	trainer_utils:	Learning rate scheduler: LinearLR
INFO:	trainer_utils:	Warmup scheduler: LinearWarmup with warmup period: 15
INFO:	trainer_utils:	Device: cuda:0
INFO:	trainer_utils:	Number of nodes: 2625
INFO:	trainer_utils:	Number of edges: 100000
INFO:	main:	Commencing training for 200 epochs
INFO:	trainer:	Epoch [1/200]	[Training]:	Loss: 50.292	Acc: 0.653	AP: 0.594	F1: 0.723	AUC: 0.653
INFO:	trainer:	Epoch [2/200]	[Training]:	Loss: 49.210	Acc: 0.698	AP: 0.630	F1: 0.739	AUC: 0.698
INFO:	trainer:	Epoch [3/200]	[Training]:	Loss: 48.187	Acc: 0.704	AP: 0.637	F1: 0.736	AUC: 0.704
INFO:	trainer:	Epoch [4/200]	[Training]:	Loss: 47.175	Acc: 0.711	AP: 0.644	F1: 0.741	AUC: 0.711
INFO:	trainer:	Epoch [5/200]	[Training]:	Loss: 46.171	Acc: 0.719	AP: 0.652	F1: 0.742	AUC: 0.719
INFO:	trainer:	Epoch [6/200]	[Training]:	Loss: 45.166	Acc: 0.725	AP: 0.659	F1: 0.742	AUC: 0.725
INFO:	trainer:	Epoch [7/200]	[Training]:	Loss: 44.174	Acc: 0.728	AP: 0.663	F1: 0.742	AUC: 0.728
INFO:	trainer:	Epoch [8/200]	[Training]:	Loss: 43.196	Acc: 0.734	AP: 0.668	F1: 0.744	AUC: 0.734
INFO:	trainer:	Epoch [9/200]	[Training]:	Loss: 42.217	Acc: 0.740	AP: 0.676	F1: 0.744	AUC: 0.740
INFO:	trainer:	Epoch [10/200]	[Training]:	Loss: 41.258	Acc: 0.737	AP: 0.674	F1: 0.740	AUC: 0.737
INFO:	trainer:	Epoch [11/200]	[Training]:	Loss: 40.297	Acc: 0.743	AP: 0.680	F1: 0.743	AUC: 0.743
INFO:	trainer:	Epoch [12/200]	[Training]:	Loss: 39.353	Acc: 0.741	AP: 0.677	F1: 0.742	AUC: 0.741
INFO:	trainer:	Epoch [13/200]	[Training]:	Loss: 38.404	Acc: 0.742	AP: 0.681	F1: 0.740	AUC: 0.742
INFO:	trainer:	Epoch [14/200]	[Training]:	Loss: 37.474	Acc: 0.742	AP: 0.682	F1: 0.735	AUC: 0.742
INFO:	trainer:	Epoch [15/200]	[Training]:	Loss: 36.548	Acc: 0.739	AP: 0.681	F1: 0.727	AUC: 0.739
INFO:	trainer:	Epoch [16/200]	[Training]:	Loss: 35.633	Acc: 0.740	AP: 0.682	F1: 0.729	AUC: 0.740
INFO:	trainer:	Epoch [17/200]	[Training]:	Loss: 34.716	Acc: 0.744	AP: 0.687	F1: 0.731	AUC: 0.744
INFO:	trainer:	Epoch [18/200]	[Training]:	Loss: 33.817	Acc: 0.740	AP: 0.682	F1: 0.726	AUC: 0.740
INFO:	trainer:	Epoch [19/200]	[Training]:	Loss: 32.923	Acc: 0.735	AP: 0.679	F1: 0.718	AUC: 0.735
INFO:	trainer:	Epoch [20/200]	[Training]:	Loss: 32.036	Acc: 0.738	AP: 0.681	F1: 0.722	AUC: 0.738
INFO:	trainer:	Epoch [21/200]	[Training]:	Loss: 31.162	Acc: 0.734	AP: 0.679	F1: 0.712	AUC: 0.734
INFO:	trainer:	Epoch [22/200]	[Training]:	Loss: 30.292	Acc: 0.735	AP: 0.681	F1: 0.710	AUC: 0.735
INFO:	trainer:	Epoch [23/200]	[Training]:	Loss: 29.432	Acc: 0.736	AP: 0.681	F1: 0.711	AUC: 0.736
INFO:	trainer:	Epoch [24/200]	[Training]:	Loss: 28.584	Acc: 0.737	AP: 0.679	F1: 0.723	AUC: 0.737
INFO:	trainer:	Epoch [25/200]	[Training]:	Loss: 27.736	Acc: 0.732	AP: 0.678	F1: 0.704	AUC: 0.732
INFO:	trainer:	Epoch [26/200]	[Training]:	Loss: 26.900	Acc: 0.736	AP: 0.682	F1: 0.708	AUC: 0.736
INFO:	trainer:	Epoch [27/200]	[Training]:	Loss: 26.076	Acc: 0.734	AP: 0.680	F1: 0.707	AUC: 0.734
INFO:	trainer:	Epoch [28/200]	[Training]:	Loss: 25.257	Acc: 0.729	AP: 0.676	F1: 0.698	AUC: 0.729
INFO:	trainer:	Epoch [29/200]	[Training]:	Loss: 24.444	Acc: 0.728	AP: 0.675	F1: 0.696	AUC: 0.728
INFO:	trainer:	Epoch [30/200]	[Training]:	Loss: 23.643	Acc: 0.723	AP: 0.671	F1: 0.688	AUC: 0.723
INFO:	trainer:	Epoch [31/200]	[Training]:	Loss: 22.850	Acc: 0.728	AP: 0.675	F1: 0.695	AUC: 0.728
INFO:	trainer:	Epoch [32/200]	[Training]:	Loss: 22.064	Acc: 0.727	AP: 0.675	F1: 0.691	AUC: 0.727
INFO:	trainer:	Epoch [33/200]	[Training]:	Loss: 21.289	Acc: 0.727	AP: 0.675	F1: 0.693	AUC: 0.727
INFO:	trainer:	Epoch [34/200]	[Training]:	Loss: 20.519	Acc: 0.729	AP: 0.677	F1: 0.692	AUC: 0.729
INFO:	trainer:	Epoch [35/200]	[Training]:	Loss: 19.764	Acc: 0.729	AP: 0.677	F1: 0.693	AUC: 0.729
INFO:	trainer:	Epoch [36/200]	[Training]:	Loss: 19.017	Acc: 0.725	AP: 0.674	F1: 0.683	AUC: 0.725
INFO:	trainer:	Epoch [37/200]	[Training]:	Loss: 18.277	Acc: 0.725	AP: 0.675	F1: 0.684	AUC: 0.725
INFO:	trainer:	Epoch [38/200]	[Training]:	Loss: 17.550	Acc: 0.722	AP: 0.672	F1: 0.679	AUC: 0.722
INFO:	trainer:	Epoch [39/200]	[Training]:	Loss: 16.832	Acc: 0.725	AP: 0.673	F1: 0.687	AUC: 0.725
INFO:	trainer:	Epoch [40/200]	[Training]:	Loss: 16.123	Acc: 0.725	AP: 0.673	F1: 0.689	AUC: 0.725
INFO:	trainer:	Epoch [41/200]	[Training]:	Loss: 15.426	Acc: 0.723	AP: 0.672	F1: 0.682	AUC: 0.723
INFO:	trainer:	Epoch [42/200]	[Training]:	Loss: 14.739	Acc: 0.720	AP: 0.670	F1: 0.676	AUC: 0.720
INFO:	trainer:	Epoch [43/200]	[Training]:	Loss: 14.061	Acc: 0.721	AP: 0.670	F1: 0.680	AUC: 0.721
INFO:	trainer:	Epoch [44/200]	[Training]:	Loss: 13.396	Acc: 0.721	AP: 0.670	F1: 0.682	AUC: 0.721
INFO:	trainer:	Epoch [45/200]	[Training]:	Loss: 12.743	Acc: 0.719	AP: 0.669	F1: 0.676	AUC: 0.719
INFO:	trainer:	Epoch [46/200]	[Training]:	Loss: 12.100	Acc: 0.723	AP: 0.673	F1: 0.679	AUC: 0.723
INFO:	trainer:	Epoch [47/200]	[Training]:	Loss: 11.469	Acc: 0.726	AP: 0.675	F1: 0.688	AUC: 0.726
INFO:	trainer:	Epoch [48/200]	[Training]:	Loss: 10.848	Acc: 0.723	AP: 0.672	F1: 0.682	AUC: 0.723
INFO:	trainer:	Epoch [49/200]	[Training]:	Loss: 10.241	Acc: 0.726	AP: 0.675	F1: 0.686	AUC: 0.726
INFO:	trainer:	Epoch [50/200]	[Training]:	Loss: 9.645	Acc: 0.725	AP: 0.674	F1: 0.686	AUC: 0.725
INFO:	trainer:	Epoch [51/200]	[Training]:	Loss: 9.062	Acc: 0.727	AP: 0.676	F1: 0.689	AUC: 0.727
INFO:	trainer:	Epoch [52/200]	[Training]:	Loss: 8.493	Acc: 0.727	AP: 0.675	F1: 0.691	AUC: 0.727
INFO:	trainer:	Epoch [53/200]	[Training]:	Loss: 7.938	Acc: 0.725	AP: 0.674	F1: 0.688	AUC: 0.725
INFO:	trainer:	Epoch [54/200]	[Training]:	Loss: 7.397	Acc: 0.725	AP: 0.674	F1: 0.686	AUC: 0.725
INFO:	trainer:	Epoch [55/200]	[Training]:	Loss: 6.870	Acc: 0.725	AP: 0.675	F1: 0.683	AUC: 0.725
INFO:	trainer:	Epoch [56/200]	[Training]:	Loss: 6.356	Acc: 0.730	AP: 0.679	F1: 0.693	AUC: 0.730
INFO:	trainer:	Epoch [57/200]	[Training]:	Loss: 5.864	Acc: 0.733	AP: 0.681	F1: 0.701	AUC: 0.733
INFO:	trainer:	Epoch [58/200]	[Training]:	Loss: 5.382	Acc: 0.730	AP: 0.679	F1: 0.696	AUC: 0.730
INFO:	trainer:	Epoch [59/200]	[Training]:	Loss: 4.919	Acc: 0.730	AP: 0.679	F1: 0.695	AUC: 0.730
INFO:	trainer:	Epoch [60/200]	[Training]:	Loss: 4.471	Acc: 0.729	AP: 0.679	F1: 0.689	AUC: 0.729
INFO:	trainer:	Epoch [61/200]	[Training]:	Loss: 4.046	Acc: 0.734	AP: 0.683	F1: 0.699	AUC: 0.734
INFO:	trainer:	Epoch [62/200]	[Training]:	Loss: 3.641	Acc: 0.731	AP: 0.680	F1: 0.693	AUC: 0.731
INFO:	trainer:	Epoch [63/200]	[Training]:	Loss: 3.257	Acc: 0.733	AP: 0.681	F1: 0.699	AUC: 0.733
INFO:	trainer:	Epoch [64/200]	[Training]:	Loss: 2.895	Acc: 0.737	AP: 0.684	F1: 0.707	AUC: 0.737
INFO:	trainer:	Epoch [65/200]	[Training]:	Loss: 2.562	Acc: 0.733	AP: 0.681	F1: 0.700	AUC: 0.733
INFO:	trainer:	Epoch [66/200]	[Training]:	Loss: 2.253	Acc: 0.735	AP: 0.682	F1: 0.702	AUC: 0.735
INFO:	trainer:	Epoch [67/200]	[Training]:	Loss: 1.977	Acc: 0.735	AP: 0.683	F1: 0.703	AUC: 0.735
INFO:	trainer:	Epoch [68/200]	[Training]:	Loss: 1.739	Acc: 0.732	AP: 0.682	F1: 0.695	AUC: 0.732
INFO:	trainer:	Epoch [69/200]	[Training]:	Loss: 1.534	Acc: 0.731	AP: 0.681	F1: 0.692	AUC: 0.731
INFO:	trainer:	Epoch [70/200]	[Training]:	Loss: 1.364	Acc: 0.734	AP: 0.681	F1: 0.701	AUC: 0.734
INFO:	trainer:	Epoch [71/200]	[Training]:	Loss: 1.227	Acc: 0.730	AP: 0.678	F1: 0.694	AUC: 0.730
INFO:	trainer:	Epoch [72/200]	[Training]:	Loss: 1.117	Acc: 0.730	AP: 0.679	F1: 0.693	AUC: 0.730
INFO:	trainer:	Epoch [73/200]	[Training]:	Loss: 1.022	Acc: 0.731	AP: 0.680	F1: 0.695	AUC: 0.731
INFO:	trainer:	Epoch [74/200]	[Training]:	Loss: 0.947	Acc: 0.732	AP: 0.681	F1: 0.695	AUC: 0.732
INFO:	trainer:	Epoch [75/200]	[Training]:	Loss: 0.876	Acc: 0.732	AP: 0.681	F1: 0.698	AUC: 0.732
INFO:	trainer:	Epoch [76/200]	[Training]:	Loss: 0.815	Acc: 0.733	AP: 0.681	F1: 0.698	AUC: 0.733
INFO:	trainer:	Epoch [77/200]	[Training]:	Loss: 0.759	Acc: 0.733	AP: 0.682	F1: 0.698	AUC: 0.733
INFO:	trainer:	Epoch [78/200]	[Training]:	Loss: 0.712	Acc: 0.731	AP: 0.678	F1: 0.700	AUC: 0.731
INFO:	trainer:	Epoch [79/200]	[Training]:	Loss: 0.666	Acc: 0.733	AP: 0.682	F1: 0.700	AUC: 0.733
INFO:	trainer:	Epoch [80/200]	[Training]:	Loss: 0.622	Acc: 0.732	AP: 0.680	F1: 0.701	AUC: 0.732
INFO:	trainer:	Epoch [81/200]	[Training]:	Loss: 0.587	Acc: 0.730	AP: 0.679	F1: 0.697	AUC: 0.730
INFO:	trainer:	Epoch [82/200]	[Training]:	Loss: 0.553	Acc: 0.732	AP: 0.681	F1: 0.699	AUC: 0.732
INFO:	trainer:	Epoch [83/200]	[Training]:	Loss: 0.527	Acc: 0.728	AP: 0.677	F1: 0.692	AUC: 0.728
INFO:	trainer:	Epoch [84/200]	[Training]:	Loss: 0.495	Acc: 0.732	AP: 0.680	F1: 0.699	AUC: 0.732
INFO:	trainer:	Epoch [85/200]	[Training]:	Loss: 0.477	Acc: 0.729	AP: 0.676	F1: 0.698	AUC: 0.729
INFO:	trainer:	Epoch [86/200]	[Training]:	Loss: 0.454	Acc: 0.731	AP: 0.680	F1: 0.699	AUC: 0.731
INFO:	trainer:	Epoch [87/200]	[Training]:	Loss: 0.435	Acc: 0.734	AP: 0.682	F1: 0.699	AUC: 0.734
INFO:	trainer:	Epoch [88/200]	[Training]:	Loss: 0.419	Acc: 0.731	AP: 0.679	F1: 0.699	AUC: 0.731
INFO:	trainer:	Epoch [89/200]	[Training]:	Loss: 0.405	Acc: 0.730	AP: 0.678	F1: 0.698	AUC: 0.730
INFO:	trainer:	Epoch [90/200]	[Training]:	Loss: 0.394	Acc: 0.730	AP: 0.679	F1: 0.698	AUC: 0.730
INFO:	trainer:	Epoch [91/200]	[Training]:	Loss: 0.382	Acc: 0.729	AP: 0.678	F1: 0.691	AUC: 0.729
INFO:	trainer:	Epoch [92/200]	[Training]:	Loss: 0.372	Acc: 0.729	AP: 0.678	F1: 0.695	AUC: 0.729
INFO:	trainer:	Epoch [93/200]	[Training]:	Loss: 0.363	Acc: 0.727	AP: 0.676	F1: 0.690	AUC: 0.727
INFO:	trainer:	Epoch [94/200]	[Training]:	Loss: 0.352	Acc: 0.734	AP: 0.682	F1: 0.702	AUC: 0.734
INFO:	trainer:	Epoch [95/200]	[Training]:	Loss: 0.352	Acc: 0.731	AP: 0.678	F1: 0.702	AUC: 0.731
INFO:	trainer:	Epoch [96/200]	[Training]:	Loss: 0.346	Acc: 0.730	AP: 0.678	F1: 0.697	AUC: 0.730
INFO:	trainer:	Epoch [97/200]	[Training]:	Loss: 0.358	Acc: 0.728	AP: 0.674	F1: 0.703	AUC: 0.728
INFO:	trainer:	Epoch [98/200]	[Training]:	Loss: 0.338	Acc: 0.732	AP: 0.681	F1: 0.696	AUC: 0.732
INFO:	trainer:	Epoch [99/200]	[Training]:	Loss: 0.335	Acc: 0.734	AP: 0.682	F1: 0.701	AUC: 0.734
INFO:	trainer:	Epoch [100/200]	[Training]:	Loss: 0.338	Acc: 0.731	AP: 0.680	F1: 0.698	AUC: 0.731
INFO:	trainer:	Epoch [101/200]	[Training]:	Loss: 0.333	Acc: 0.732	AP: 0.682	F1: 0.696	AUC: 0.732
INFO:	trainer:	Epoch [102/200]	[Training]:	Loss: 0.329	Acc: 0.733	AP: 0.682	F1: 0.699	AUC: 0.733
INFO:	trainer:	Epoch [103/200]	[Training]:	Loss: 0.330	Acc: 0.730	AP: 0.680	F1: 0.693	AUC: 0.730
INFO:	trainer:	Epoch [104/200]	[Training]:	Loss: 0.330	Acc: 0.730	AP: 0.680	F1: 0.692	AUC: 0.730
INFO:	trainer:	Epoch [105/200]	[Training]:	Loss: 0.325	Acc: 0.730	AP: 0.680	F1: 0.691	AUC: 0.730
INFO:	trainer:	Epoch [106/200]	[Training]:	Loss: 0.325	Acc: 0.731	AP: 0.682	F1: 0.691	AUC: 0.731
INFO:	trainer:	Epoch [107/200]	[Training]:	Loss: 0.329	Acc: 0.727	AP: 0.678	F1: 0.687	AUC: 0.727
INFO:	trainer:	Epoch [108/200]	[Training]:	Loss: 0.329	Acc: 0.727	AP: 0.678	F1: 0.686	AUC: 0.727
INFO:	trainer:	Epoch [109/200]	[Training]:	Loss: 0.327	Acc: 0.731	AP: 0.681	F1: 0.692	AUC: 0.731
INFO:	trainer:	Epoch [110/200]	[Training]:	Loss: 0.322	Acc: 0.733	AP: 0.684	F1: 0.693	AUC: 0.733
INFO:	trainer:	Epoch [111/200]	[Training]:	Loss: 0.327	Acc: 0.728	AP: 0.680	F1: 0.685	AUC: 0.728
INFO:	trainer:	Epoch [112/200]	[Training]:	Loss: 0.323	Acc: 0.731	AP: 0.681	F1: 0.691	AUC: 0.731
INFO:	trainer:	Epoch [113/200]	[Training]:	Loss: 0.323	Acc: 0.735	AP: 0.685	F1: 0.698	AUC: 0.735
INFO:	trainer:	Epoch [114/200]	[Training]:	Loss: 0.322	Acc: 0.731	AP: 0.683	F1: 0.685	AUC: 0.731
INFO:	trainer:	Epoch [115/200]	[Training]:	Loss: 0.320	Acc: 0.731	AP: 0.682	F1: 0.688	AUC: 0.731
INFO:	trainer:	Epoch [116/200]	[Training]:	Loss: 0.320	Acc: 0.732	AP: 0.684	F1: 0.689	AUC: 0.732
INFO:	trainer:	Epoch [117/200]	[Training]:	Loss: 0.318	Acc: 0.733	AP: 0.686	F1: 0.690	AUC: 0.733
INFO:	trainer:	Epoch [118/200]	[Training]:	Loss: 0.323	Acc: 0.731	AP: 0.683	F1: 0.689	AUC: 0.731
INFO:	trainer:	Epoch [119/200]	[Training]:	Loss: 0.320	Acc: 0.733	AP: 0.686	F1: 0.690	AUC: 0.733
INFO:	trainer:	Epoch [120/200]	[Training]:	Loss: 0.319	Acc: 0.731	AP: 0.683	F1: 0.685	AUC: 0.731
INFO:	trainer:	Epoch [121/200]	[Training]:	Loss: 0.317	Acc: 0.734	AP: 0.687	F1: 0.690	AUC: 0.734
INFO:	trainer:	Epoch [122/200]	[Training]:	Loss: 0.318	Acc: 0.730	AP: 0.684	F1: 0.682	AUC: 0.730
INFO:	trainer:	Epoch [123/200]	[Training]:	Loss: 0.317	Acc: 0.732	AP: 0.686	F1: 0.684	AUC: 0.732
INFO:	trainer:	Epoch [124/200]	[Training]:	Loss: 0.317	Acc: 0.729	AP: 0.684	F1: 0.680	AUC: 0.729
INFO:	trainer:	Epoch [125/200]	[Training]:	Loss: 0.315	Acc: 0.735	AP: 0.690	F1: 0.687	AUC: 0.735
INFO:	trainer:	Epoch [126/200]	[Training]:	Loss: 0.320	Acc: 0.737	AP: 0.689	F1: 0.695	AUC: 0.737
INFO:	trainer:	Epoch [127/200]	[Training]:	Loss: 0.315	Acc: 0.734	AP: 0.688	F1: 0.688	AUC: 0.734
INFO:	trainer:	Epoch [128/200]	[Training]:	Loss: 0.314	Acc: 0.735	AP: 0.689	F1: 0.688	AUC: 0.735
INFO:	trainer:	Epoch [129/200]	[Training]:	Loss: 0.316	Acc: 0.732	AP: 0.685	F1: 0.684	AUC: 0.732
INFO:	trainer:	Epoch [130/200]	[Training]:	Loss: 0.314	Acc: 0.738	AP: 0.693	F1: 0.694	AUC: 0.738
INFO:	trainer:	Epoch [131/200]	[Training]:	Loss: 0.314	Acc: 0.737	AP: 0.693	F1: 0.690	AUC: 0.737
INFO:	trainer:	Epoch [132/200]	[Training]:	Loss: 0.314	Acc: 0.736	AP: 0.691	F1: 0.690	AUC: 0.736
INFO:	trainer:	Epoch [133/200]	[Training]:	Loss: 0.317	Acc: 0.737	AP: 0.693	F1: 0.689	AUC: 0.737
INFO:	trainer:	Epoch [134/200]	[Training]:	Loss: 0.312	Acc: 0.736	AP: 0.691	F1: 0.689	AUC: 0.736
INFO:	trainer:	Epoch [135/200]	[Training]:	Loss: 0.314	Acc: 0.737	AP: 0.691	F1: 0.691	AUC: 0.737
INFO:	trainer:	Epoch [136/200]	[Training]:	Loss: 0.312	Acc: 0.740	AP: 0.694	F1: 0.694	AUC: 0.740
INFO:	trainer:	Epoch [137/200]	[Training]:	Loss: 0.311	Acc: 0.740	AP: 0.695	F1: 0.694	AUC: 0.740
INFO:	trainer:	Epoch [138/200]	[Training]:	Loss: 0.308	Acc: 0.743	AP: 0.698	F1: 0.698	AUC: 0.743
INFO:	trainer:	Epoch [139/200]	[Training]:	Loss: 0.311	Acc: 0.741	AP: 0.695	F1: 0.698	AUC: 0.741
INFO:	trainer:	Epoch [140/200]	[Training]:	Loss: 0.308	Acc: 0.743	AP: 0.697	F1: 0.702	AUC: 0.743
INFO:	trainer:	Epoch [141/200]	[Training]:	Loss: 0.307	Acc: 0.747	AP: 0.700	F1: 0.710	AUC: 0.747
INFO:	trainer:	Epoch [142/200]	[Training]:	Loss: 0.309	Acc: 0.746	AP: 0.700	F1: 0.705	AUC: 0.746
INFO:	trainer:	Epoch [143/200]	[Training]:	Loss: 0.309	Acc: 0.744	AP: 0.698	F1: 0.703	AUC: 0.744
INFO:	trainer:	Epoch [144/200]	[Training]:	Loss: 0.306	Acc: 0.747	AP: 0.701	F1: 0.708	AUC: 0.747
INFO:	trainer:	Epoch [145/200]	[Training]:	Loss: 0.310	Acc: 0.742	AP: 0.697	F1: 0.700	AUC: 0.742
INFO:	trainer:	Epoch [146/200]	[Training]:	Loss: 0.308	Acc: 0.745	AP: 0.700	F1: 0.703	AUC: 0.745
INFO:	trainer:	Epoch [147/200]	[Training]:	Loss: 0.309	Acc: 0.743	AP: 0.697	F1: 0.702	AUC: 0.743
INFO:	trainer:	Epoch [148/200]	[Training]:	Loss: 0.305	Acc: 0.744	AP: 0.698	F1: 0.703	AUC: 0.744
INFO:	trainer:	Epoch [149/200]	[Training]:	Loss: 0.309	Acc: 0.744	AP: 0.698	F1: 0.703	AUC: 0.744
INFO:	trainer:	Epoch [150/200]	[Training]:	Loss: 0.308	Acc: 0.744	AP: 0.698	F1: 0.702	AUC: 0.744
INFO:	trainer:	Epoch [151/200]	[Training]:	Loss: 0.305	Acc: 0.743	AP: 0.696	F1: 0.702	AUC: 0.743
INFO:	trainer:	Epoch [152/200]	[Training]:	Loss: 0.304	Acc: 0.747	AP: 0.702	F1: 0.707	AUC: 0.747
INFO:	trainer:	Epoch [153/200]	[Training]:	Loss: 0.307	Acc: 0.741	AP: 0.697	F1: 0.696	AUC: 0.741
INFO:	trainer:	Epoch [154/200]	[Training]:	Loss: 0.305	Acc: 0.745	AP: 0.698	F1: 0.706	AUC: 0.745
INFO:	trainer:	Epoch [155/200]	[Training]:	Loss: 0.306	Acc: 0.743	AP: 0.697	F1: 0.700	AUC: 0.743
INFO:	trainer:	Epoch [156/200]	[Training]:	Loss: 0.309	Acc: 0.742	AP: 0.698	F1: 0.695	AUC: 0.742
INFO:	trainer:	Epoch [157/200]	[Training]:	Loss: 0.303	Acc: 0.743	AP: 0.700	F1: 0.698	AUC: 0.743
INFO:	trainer:	Epoch [158/200]	[Training]:	Loss: 0.305	Acc: 0.743	AP: 0.698	F1: 0.700	AUC: 0.743
INFO:	trainer:	Epoch [159/200]	[Training]:	Loss: 0.302	Acc: 0.746	AP: 0.700	F1: 0.705	AUC: 0.746
INFO:	trainer:	Epoch [160/200]	[Training]:	Loss: 0.306	Acc: 0.744	AP: 0.699	F1: 0.703	AUC: 0.744
INFO:	trainer:	Epoch [161/200]	[Training]:	Loss: 0.305	Acc: 0.745	AP: 0.700	F1: 0.702	AUC: 0.745
INFO:	trainer:	Epoch [162/200]	[Training]:	Loss: 0.302	Acc: 0.747	AP: 0.701	F1: 0.706	AUC: 0.747
INFO:	trainer:	Epoch [163/200]	[Training]:	Loss: 0.306	Acc: 0.747	AP: 0.702	F1: 0.706	AUC: 0.747
INFO:	trainer:	Epoch [164/200]	[Training]:	Loss: 0.301	Acc: 0.746	AP: 0.701	F1: 0.705	AUC: 0.746
INFO:	trainer:	Epoch [165/200]	[Training]:	Loss: 0.304	Acc: 0.748	AP: 0.702	F1: 0.710	AUC: 0.748
INFO:	trainer:	Epoch [166/200]	[Training]:	Loss: 0.300	Acc: 0.745	AP: 0.700	F1: 0.703	AUC: 0.745
INFO:	trainer:	Epoch [167/200]	[Training]:	Loss: 0.303	Acc: 0.750	AP: 0.702	F1: 0.713	AUC: 0.750
INFO:	trainer:	Epoch [168/200]	[Training]:	Loss: 0.301	Acc: 0.751	AP: 0.705	F1: 0.713	AUC: 0.751
INFO:	trainer:	Epoch [169/200]	[Training]:	Loss: 0.304	Acc: 0.745	AP: 0.700	F1: 0.702	AUC: 0.745
INFO:	trainer:	Epoch [170/200]	[Training]:	Loss: 0.303	Acc: 0.746	AP: 0.702	F1: 0.701	AUC: 0.746
INFO:	trainer:	Epoch [171/200]	[Training]:	Loss: 0.302	Acc: 0.747	AP: 0.702	F1: 0.704	AUC: 0.747
INFO:	trainer:	Epoch [172/200]	[Training]:	Loss: 0.299	Acc: 0.749	AP: 0.702	F1: 0.713	AUC: 0.749
INFO:	trainer:	Epoch [173/200]	[Training]:	Loss: 0.299	Acc: 0.746	AP: 0.702	F1: 0.701	AUC: 0.746
INFO:	trainer:	Epoch [174/200]	[Training]:	Loss: 0.298	Acc: 0.750	AP: 0.704	F1: 0.711	AUC: 0.750
INFO:	trainer:	Epoch [175/200]	[Training]:	Loss: 0.300	Acc: 0.754	AP: 0.707	F1: 0.718	AUC: 0.754
INFO:	trainer:	Epoch [176/200]	[Training]:	Loss: 0.299	Acc: 0.749	AP: 0.703	F1: 0.708	AUC: 0.749
INFO:	trainer:	Epoch [177/200]	[Training]:	Loss: 0.300	Acc: 0.752	AP: 0.708	F1: 0.714	AUC: 0.752
INFO:	trainer:	Epoch [178/200]	[Training]:	Loss: 0.297	Acc: 0.752	AP: 0.706	F1: 0.714	AUC: 0.752
INFO:	trainer:	Epoch [179/200]	[Training]:	Loss: 0.300	Acc: 0.748	AP: 0.703	F1: 0.708	AUC: 0.748
INFO:	trainer:	Epoch [180/200]	[Training]:	Loss: 0.298	Acc: 0.754	AP: 0.707	F1: 0.719	AUC: 0.754
INFO:	trainer:	Epoch [181/200]	[Training]:	Loss: 0.298	Acc: 0.748	AP: 0.704	F1: 0.704	AUC: 0.748
INFO:	trainer:	Epoch [182/200]	[Training]:	Loss: 0.299	Acc: 0.751	AP: 0.704	F1: 0.712	AUC: 0.751
INFO:	trainer:	Epoch [183/200]	[Training]:	Loss: 0.300	Acc: 0.751	AP: 0.705	F1: 0.714	AUC: 0.751
INFO:	trainer:	Epoch [184/200]	[Training]:	Loss: 0.298	Acc: 0.751	AP: 0.704	F1: 0.713	AUC: 0.751
INFO:	trainer:	Epoch [185/200]	[Training]:	Loss: 0.297	Acc: 0.750	AP: 0.704	F1: 0.710	AUC: 0.750
INFO:	trainer:	Epoch [186/200]	[Training]:	Loss: 0.298	Acc: 0.752	AP: 0.705	F1: 0.715	AUC: 0.752
INFO:	trainer:	Epoch [187/200]	[Training]:	Loss: 0.297	Acc: 0.750	AP: 0.705	F1: 0.710	AUC: 0.750
INFO:	trainer:	Epoch [188/200]	[Training]:	Loss: 0.297	Acc: 0.751	AP: 0.706	F1: 0.713	AUC: 0.751
INFO:	trainer:	Epoch [189/200]	[Training]:	Loss: 0.298	Acc: 0.750	AP: 0.705	F1: 0.711	AUC: 0.750
INFO:	trainer:	Epoch [190/200]	[Training]:	Loss: 0.299	Acc: 0.750	AP: 0.704	F1: 0.712	AUC: 0.750
INFO:	trainer:	Epoch [191/200]	[Training]:	Loss: 0.301	Acc: 0.753	AP: 0.705	F1: 0.721	AUC: 0.753
INFO:	trainer:	Epoch [192/200]	[Training]:	Loss: 0.299	Acc: 0.752	AP: 0.704	F1: 0.715	AUC: 0.752
INFO:	trainer:	Epoch [193/200]	[Training]:	Loss: 0.296	Acc: 0.751	AP: 0.705	F1: 0.712	AUC: 0.751
INFO:	trainer:	Epoch [194/200]	[Training]:	Loss: 0.298	Acc: 0.749	AP: 0.703	F1: 0.711	AUC: 0.749
INFO:	trainer:	Epoch [195/200]	[Training]:	Loss: 0.297	Acc: 0.753	AP: 0.707	F1: 0.716	AUC: 0.753
INFO:	trainer:	Epoch [196/200]	[Training]:	Loss: 0.298	Acc: 0.751	AP: 0.705	F1: 0.716	AUC: 0.751
INFO:	trainer:	Epoch [197/200]	[Training]:	Loss: 0.298	Acc: 0.755	AP: 0.706	F1: 0.724	AUC: 0.755
INFO:	trainer:	Epoch [198/200]	[Training]:	Loss: 0.297	Acc: 0.753	AP: 0.706	F1: 0.718	AUC: 0.753
INFO:	trainer:	Epoch [199/200]	[Training]:	Loss: 0.296	Acc: 0.753	AP: 0.706	F1: 0.716	AUC: 0.753
INFO:	trainer:	Epoch [200/200]	[Training]:	Loss: 0.292	Acc: 0.756	AP: 0.710	F1: 0.723	AUC: 0.756
INFO:	trainer:	Epoch [200/200]	[Validation]:	R@10: 0.059	R@20: 0.097	MRR: 0.024
