INFO:	main:	Namespace(n_degree=20, n_head=2, n_layer=2, drop_out=0.1, reg=0.1, node_dim=32, time_dim=32, agg_method='attn', attn_mode='map', uniform=True, new_node=False, time='disentangle', disencomponents=10, data='ml-100k', samplerate=1.0, popnegsample=False, timepopnegsample=False, negsampleeval=1000, bs=256, n_epoch=200, lr=0.001, prefix='ml100k', gpu=0, seed=42, l2=0.0001)
INFO:	TGSRec:	Aggregation uses attention model
INFO:	attention_model:	Using map based attention
INFO:	attention_model:	Using map based attention
INFO:	TGSRec:	Using disentangle time encoding
INFO:	trainer:	Model built successfully
INFO:	trainer:	TGRec(
  (node_hist_embed): Embedding(2626, 32)
  (merge_layer): MergeLayer(
    (fc1): Linear(in_features=64, out_features=32, bias=True)
    (fc2): Linear(in_features=32, out_features=32, bias=True)
    (act): ReLU()
  )
  (attn_model_list): ModuleList(
    (0-1): 2 x AttnModel(
      (merger): MergeLayer(
        (fc1): Linear(in_features=96, out_features=32, bias=True)
        (fc2): Linear(in_features=32, out_features=32, bias=True)
        (act): ReLU()
      )
      (multi_head_target): MapBasedMultiHeadAttention(
        (wq_node_transform): Linear(in_features=64, out_features=64, bias=False)
        (wk_node_transform): Linear(in_features=64, out_features=64, bias=False)
        (wv_node_transform): Linear(in_features=64, out_features=64, bias=False)
        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (fc): Linear(in_features=64, out_features=64, bias=True)
        (act): LeakyReLU(negative_slope=0.2)
        (weight_map): Linear(in_features=64, out_features=1, bias=False)
        (dropout): Dropout(p=0.1, inplace=False)
        (softmax): Softmax(dim=2)
      )
    )
  )
  (time_encoder): DisentangleTimeEncode()
  (affinity_score): MergeLayer(
    (fc1): Linear(in_features=64, out_features=32, bias=True)
    (fc2): Linear(in_features=32, out_features=1, bias=True)
    (act): ReLU()
  )
)
INFO:	trainer:	Optimizer: Adam with lr: 0.001 and l2: 0.0001
INFO:	trainer:	Learning rate scheduler: LinearLR
INFO:	trainer:	Warmup scheduler: LinearWarmup
INFO:	trainer:	Device: cuda:0
INFO:	trainer:	Number of nodes: 2625
INFO:	trainer:	Number of edges: 100000
INFO:	callbacks:	Early stopping monitor: max_round=3, higher_better=True, tolerance=0.001
INFO:	main:	Commencing training for 200 epochs
INFO:	trainer:	Epoch [1/200]:	Train Loss: 13.9613	Train Acc: 0.6611	Train AP: 0.6975	Train F1: 0.7303	Train AUC: 0.7426	Val Acc: 0.5100	Val AP: 0.7179	Val F1: 0.0467	Val AUC: 0.7542	Test Acc: 0.5037	Test AP: 0.7490	Test F1: 0.0215	Test AUC: 0.7880
INFO:	trainer:	Epoch [2/200]:	Train Loss: 7.6280	Train Acc: 0.6696	Train AP: 0.7712	Train F1: 0.7366	Train AUC: 0.7996	Val Acc: 0.5680	Val AP: 0.7440	Val F1: 0.2676	Val AUC: 0.7622	Test Acc: 0.5471	Test AP: 0.7624	Test F1: 0.1870	Test AUC: 0.7829
INFO:	trainer:	Epoch [3/200]:	Train Loss: 2.7230	Train Acc: 0.6689	Train AP: 0.8162	Train F1: 0.7394	Train AUC: 0.8299	Val Acc: 0.6355	Val AP: 0.7799	Val F1: 0.4880	Val AUC: 0.7886	Test Acc: 0.5898	Test AP: 0.7927	Test F1: 0.3296	Test AUC: 0.8193
INFO:	trainer:	Epoch [4/200]:	Train Loss: 0.5806	Train Acc: 0.6867	Train AP: 0.8181	Train F1: 0.7435	Train AUC: 0.8331	Val Acc: 0.6592	Val AP: 0.7688	Val F1: 0.5696	Val AUC: 0.7729	Test Acc: 0.6261	Test AP: 0.7350	Test F1: 0.4864	Test AUC: 0.7700
INFO:	trainer:	Epoch [5/200]:	Train Loss: 0.4628	Train Acc: 0.7008	Train AP: 0.8188	Train F1: 0.7448	Train AUC: 0.8349	Val Acc: 0.6775	Val AP: 0.7727	Val F1: 0.6227	Val AUC: 0.7807	Test Acc: 0.6553	Test AP: 0.7710	Test F1: 0.5612	Test AUC: 0.7903
INFO:	trainer:	Early stopping at epoch 6
INFO:	main:	Training finished
INFO:	main:	Plotting results
INFO:	main:	Plotting finished
INFO:	trainer:	History exported to ./log/140324-044705_history.json
INFO:	main:	Completed main file execution. Exiting...
