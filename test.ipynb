{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/FYP/siddhant005/.conda/envs/retagnn_pyg/bin/python\n",
      "GPU 0: Tesla V100-PCIE-32GB (UUID: GPU-0a7c46ac-ace6-42ba-35cf-73fd8d6d7cf8)\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:\tmain:\tNamespace(n_degree=30, n_head=2, n_layer=2, drop_out=0.3, reg=0.3, node_dim=32, time_dim=32, agg_method='attn', attn_mode='map', uniform=True, new_node=False, time='disentangle', disencomponents=10, data='ml-100k', train_test_val='1-1-98', samplerate=1.0, popnegsample=False, timepopnegsample=False, negsampleeval=100, bs=128, n_epoch=1, lr=0.0001, prefix='ml100k', gpu=0, seed=42, l2=0.0001, ckpt_epoch=10)\n",
      "INFO:\tdata_utils:\tTrain split: 1%, Val split: 1%, Test split: 98%\n",
      "INFO:\tTGSRec:\tAggregation uses attention model\n",
      "INFO:\tTGSRec:\tUsing disentangle time encoding\n",
      "INFO:\ttrainer_utils:\tModel built successfully\n",
      "INFO:\ttrainer_utils:\tTGRec(\n",
      "  (node_hist_embed): Embedding(2626, 32)\n",
      "  (merge_layer): MergeLayer(\n",
      "    (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (attn_model_list): ModuleList(\n",
      "    (0-1): 2 x CrossAttentionModel(\n",
      "      (ffn): MergeLayer(\n",
      "        (fc1): Linear(in_features=96, out_features=32, bias=True)\n",
      "        (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (multi_head_target): MapBasedMultiHeadAttention(\n",
      "        (wq_node_transform): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (wk_node_transform): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (wv_node_transform): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (act): LeakyReLU(negative_slope=0.2)\n",
      "        (weight_map): Linear(in_features=64, out_features=1, bias=False)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "        (softmax): Softmax(dim=2)\n",
      "      )\n",
      "      (self_attn_model): SelfAttentionModel(\n",
      "        (ffn): MergeSelfAttnLayer(\n",
      "          (fc1): Linear(in_features=96, out_features=3840, bias=True)\n",
      "          (fc2): Linear(in_features=3840, out_features=64, bias=True)\n",
      "          (act): ReLU()\n",
      "        )\n",
      "        (multi_head_target): MultiHeadAttention(\n",
      "          (w_qs): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (w_ks): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (w_vs): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (softmax): Softmax(dim=2)\n",
      "          )\n",
      "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (time_encoder): DisentangleTimeEncode()\n",
      "  (affinity_score): MergeLayer(\n",
      "    (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (act): ReLU()\n",
      "  )\n",
      ")\n",
      "INFO:\ttrainer_utils:\tNumber of parameters: 1402305 (trainable: 1402305)\n",
      "INFO:\ttrainer_utils:\tLoss function: BPR + L2 regularization\n",
      "INFO:\ttrainer_utils:\tOptimizer: Adam with lr: 0.0001 and l2: 0.0001\n",
      "INFO:\ttrainer_utils:\tLearning rate scheduler: LinearLR\n",
      "INFO:\ttrainer_utils:\tWarmup scheduler: LinearWarmup with warmup period: 15\n",
      "INFO:\ttrainer_utils:\tDevice: cuda:0\n",
      "INFO:\ttrainer_utils:\tNumber of nodes: 2625\n",
      "INFO:\ttrainer_utils:\tNumber of edges: 100000\n",
      "INFO:\tmain:\tCommencing training for 1 epochs\n",
      "INFO:\ttrainer:\tEpoch [1/1]\t[Training]:\tTime: 3.38sec\tLoss: 51.0203\tAcc: 0.4982\tAP: 0.4999\tF1: 0.5114\tAUC: 0.4982\n",
      "INFO:\ttrainer:\tEpoch [1/1]\t[Validation]:\tTime: 22.65sec\tR@10: 0.0192\tR@20: 0.0449\tMRR: 0.0160\n",
      "test_results: {'recall': array([0.01960784, 0.04374057]), 'ndcg': array([0.01277785, 0.0189957 ]), 'mrr': 0.0125808465101434}\n",
      "INFO:\ttrainer:\tFinal Test:\tTime: 117.34sec\tR@10: 0.0196\tR@20: 0.0437\tMRR: 0.0126\n",
      "INFO:\ttrainer:\tHistory exported to /home/FYP/siddhant005/fyp/log/150324-185839_history.json\n",
      "INFO:\ttrainer:\tTraining completed, model and optimizer state saved locally.\n",
      "INFO:\tmain:\tTraining finished. Completed main file execution. Exiting...\n"
     ]
    }
   ],
   "source": [
    "!python /home/FYP/siddhant005/fyp/src/main.py -d ml-100k --uniform --bs 128 --n_epoch 1 --train_test_val 1-1-98 --negsampleeval 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retagnn_pyg",
   "language": "python",
   "name": "retagnn_pyg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
