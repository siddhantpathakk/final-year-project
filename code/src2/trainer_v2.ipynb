{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-PCIE-32GB (UUID: GPU-6836cac4-6472-db93-c539-eaa93a70b15b)\n",
      "/home/FYP/siddhant005/.conda/envs/retagnn_pyg/bin/python\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L\n",
    "! which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movielens-100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 1,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run1/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run1\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 1 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_layer): CrossAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 0.9920\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0371\tNDCG@10: 0.0966\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 0.9734\tP@10: 0.0363\tR@10: 0.0181\tMAP@10: 0.0094\tNDCG@10: 0.0329\tHR@10: 0.3204\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 1.1067\tP@10: 0.0470\tR@10: 0.0235\tMAP@10: 0.0151\tNDCG@10: 0.0470\tHR@10: 0.3908\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 1.0457\tP@10: 0.0683\tR@10: 0.0342\tMAP@10: 0.0258\tNDCG@10: 0.0719\tHR@10: 0.4965\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 0.9191\tP@10: 0.0764\tR@10: 0.0382\tMAP@10: 0.0249\tNDCG@10: 0.0739\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 0.9586\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0347\tNDCG@10: 0.0927\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 0.8983\tP@10: 0.0815\tR@10: 0.0408\tMAP@10: 0.0312\tNDCG@10: 0.0850\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 0.8602\tP@10: 0.0924\tR@10: 0.0462\tMAP@10: 0.0341\tNDCG@10: 0.0927\tHR@10: 0.6056\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 0.8320\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0337\tNDCG@10: 0.0896\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 0.8066\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0351\tNDCG@10: 0.0929\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 0.7955\tP@10: 0.0806\tR@10: 0.0403\tMAP@10: 0.0345\tNDCG@10: 0.0903\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 12/500:\tLoss: 0.7869\tP@10: 0.0910\tR@10: 0.0455\tMAP@10: 0.0349\tNDCG@10: 0.0947\tHR@10: 0.6180\n",
      "INFO:\ttrainer:\tEpoch 13/500:\tLoss: 0.7776\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0319\tNDCG@10: 0.0890\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 14/500:\tLoss: 0.7592\tP@10: 0.0729\tR@10: 0.0364\tMAP@10: 0.0293\tNDCG@10: 0.0793\tHR@10: 0.5176\n",
      "INFO:\ttrainer:\tEpoch 15/500:\tLoss: 0.7524\tP@10: 0.0792\tR@10: 0.0396\tMAP@10: 0.0275\tNDCG@10: 0.0773\tHR@10: 0.5299\n",
      "INFO:\ttrainer:\tEpoch 16/500:\tLoss: 0.7294\tP@10: 0.0796\tR@10: 0.0398\tMAP@10: 0.0305\tNDCG@10: 0.0821\tHR@10: 0.5370\n",
      "INFO:\ttrainer:\tEpoch 17/500:\tLoss: 0.7129\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0344\tNDCG@10: 0.0907\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 18/500:\tLoss: 0.7067\tP@10: 0.0847\tR@10: 0.0423\tMAP@10: 0.0325\tNDCG@10: 0.0875\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 19/500:\tLoss: 0.6996\tP@10: 0.0776\tR@10: 0.0388\tMAP@10: 0.0287\tNDCG@10: 0.0798\tHR@10: 0.5440\n",
      "INFO:\ttrainer:\tEpoch 20/500:\tLoss: 0.6899\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0315\tNDCG@10: 0.0881\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 21/500:\tLoss: 0.6821\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0314\tNDCG@10: 0.0866\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 22/500:\tLoss: 0.6736\tP@10: 0.0877\tR@10: 0.0438\tMAP@10: 0.0292\tNDCG@10: 0.0831\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 23/500:\tLoss: 0.6564\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0324\tNDCG@10: 0.0891\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 24/500:\tLoss: 0.6459\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0284\tNDCG@10: 0.0814\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 25/500:\tLoss: 0.6416\tP@10: 0.0780\tR@10: 0.0390\tMAP@10: 0.0251\tNDCG@10: 0.0743\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 26/500:\tLoss: 0.6309\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0285\tNDCG@10: 0.0831\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 27/500:\tLoss: 0.6284\tP@10: 0.0868\tR@10: 0.0434\tMAP@10: 0.0291\tNDCG@10: 0.0835\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 28/500:\tLoss: 0.6208\tP@10: 0.0931\tR@10: 0.0466\tMAP@10: 0.0345\tNDCG@10: 0.0943\tHR@10: 0.6074\n",
      "INFO:\ttrainer:\tEpoch 29/500:\tLoss: 0.6181\tP@10: 0.0947\tR@10: 0.0474\tMAP@10: 0.0344\tNDCG@10: 0.0943\tHR@10: 0.6092\n",
      "INFO:\ttrainer:\tEpoch 30/500:\tLoss: 0.6086\tP@10: 0.0820\tR@10: 0.0410\tMAP@10: 0.0311\tNDCG@10: 0.0841\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 31/500:\tLoss: 0.6175\tP@10: 0.0861\tR@10: 0.0430\tMAP@10: 0.0329\tNDCG@10: 0.0895\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 32/500:\tLoss: 0.6114\tP@10: 0.0840\tR@10: 0.0420\tMAP@10: 0.0308\tNDCG@10: 0.0849\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 33/500:\tLoss: 0.5993\tP@10: 0.0903\tR@10: 0.0452\tMAP@10: 0.0334\tNDCG@10: 0.0918\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 34/500:\tLoss: 0.6004\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0322\tNDCG@10: 0.0875\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 35/500:\tLoss: 0.5958\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0327\tNDCG@10: 0.0898\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 36/500:\tLoss: 0.5899\tP@10: 0.0868\tR@10: 0.0434\tMAP@10: 0.0320\tNDCG@10: 0.0888\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 37/500:\tLoss: 0.5874\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0334\tNDCG@10: 0.0914\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 38/500:\tLoss: 0.5866\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0322\tNDCG@10: 0.0894\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 39/500:\tLoss: 0.5869\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0320\tNDCG@10: 0.0868\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 40/500:\tLoss: 0.5841\tP@10: 0.0801\tR@10: 0.0401\tMAP@10: 0.0321\tNDCG@10: 0.0853\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 41/500:\tLoss: 0.5758\tP@10: 0.0827\tR@10: 0.0414\tMAP@10: 0.0330\tNDCG@10: 0.0873\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 42/500:\tLoss: 0.5782\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0309\tNDCG@10: 0.0855\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 43/500:\tLoss: 0.5822\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0322\tNDCG@10: 0.0889\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 44/500:\tLoss: 0.5746\tP@10: 0.0820\tR@10: 0.0410\tMAP@10: 0.0325\tNDCG@10: 0.0875\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 45/500:\tLoss: 0.5652\tP@10: 0.0824\tR@10: 0.0412\tMAP@10: 0.0323\tNDCG@10: 0.0878\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 46/500:\tLoss: 0.5697\tP@10: 0.0820\tR@10: 0.0410\tMAP@10: 0.0278\tNDCG@10: 0.0803\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 47/500:\tLoss: 0.5669\tP@10: 0.0761\tR@10: 0.0380\tMAP@10: 0.0234\tNDCG@10: 0.0706\tHR@10: 0.5317\n",
      "INFO:\ttrainer:\tEpoch 48/500:\tLoss: 0.5699\tP@10: 0.0822\tR@10: 0.0411\tMAP@10: 0.0304\tNDCG@10: 0.0848\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 49/500:\tLoss: 0.5667\tP@10: 0.0796\tR@10: 0.0398\tMAP@10: 0.0293\tNDCG@10: 0.0820\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 50/500:\tLoss: 0.5637\tP@10: 0.0847\tR@10: 0.0423\tMAP@10: 0.0320\tNDCG@10: 0.0870\tHR@10: 0.5493\n",
      "INFO:\ttrainer:\tEpoch 51/500:\tLoss: 0.5630\tP@10: 0.0843\tR@10: 0.0422\tMAP@10: 0.0326\tNDCG@10: 0.0877\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 52/500:\tLoss: 0.5631\tP@10: 0.0817\tR@10: 0.0408\tMAP@10: 0.0295\tNDCG@10: 0.0818\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 53/500:\tLoss: 0.5561\tP@10: 0.0776\tR@10: 0.0388\tMAP@10: 0.0311\tNDCG@10: 0.0832\tHR@10: 0.5370\n",
      "INFO:\ttrainer:\tEpoch 54/500:\tLoss: 0.5497\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0356\tNDCG@10: 0.0943\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 55/500:\tLoss: 0.5530\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0351\tNDCG@10: 0.0924\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 56/500:\tLoss: 0.5506\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0339\tNDCG@10: 0.0901\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 57/500:\tLoss: 0.5483\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0321\tNDCG@10: 0.0873\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 58/500:\tLoss: 0.5404\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0278\tNDCG@10: 0.0817\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 59/500:\tLoss: 0.5502\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0319\tNDCG@10: 0.0886\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 60/500:\tLoss: 0.5548\tP@10: 0.0917\tR@10: 0.0459\tMAP@10: 0.0347\tNDCG@10: 0.0941\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 61/500:\tLoss: 0.5473\tP@10: 0.0859\tR@10: 0.0430\tMAP@10: 0.0336\tNDCG@10: 0.0900\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 62/500:\tLoss: 0.5452\tP@10: 0.0829\tR@10: 0.0415\tMAP@10: 0.0315\tNDCG@10: 0.0868\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 63/500:\tLoss: 0.5462\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0328\tNDCG@10: 0.0906\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 64/500:\tLoss: 0.5537\tP@10: 0.0903\tR@10: 0.0452\tMAP@10: 0.0346\tNDCG@10: 0.0929\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 65/500:\tLoss: 0.5528\tP@10: 0.0789\tR@10: 0.0394\tMAP@10: 0.0320\tNDCG@10: 0.0847\tHR@10: 0.5352\n",
      "INFO:\ttrainer:\tEpoch 66/500:\tLoss: 0.5485\tP@10: 0.0861\tR@10: 0.0430\tMAP@10: 0.0328\tNDCG@10: 0.0885\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 67/500:\tLoss: 0.5384\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0324\tNDCG@10: 0.0876\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 68/500:\tLoss: 0.5392\tP@10: 0.0882\tR@10: 0.0441\tMAP@10: 0.0336\tNDCG@10: 0.0903\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 69/500:\tLoss: 0.5431\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0332\tNDCG@10: 0.0895\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 70/500:\tLoss: 0.5375\tP@10: 0.0907\tR@10: 0.0453\tMAP@10: 0.0343\tNDCG@10: 0.0925\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 71/500:\tLoss: 0.5428\tP@10: 0.0863\tR@10: 0.0431\tMAP@10: 0.0331\tNDCG@10: 0.0896\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 72/500:\tLoss: 0.5407\tP@10: 0.0884\tR@10: 0.0442\tMAP@10: 0.0337\tNDCG@10: 0.0912\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 73/500:\tLoss: 0.5346\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0343\tNDCG@10: 0.0927\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 74/500:\tLoss: 0.5287\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0326\tNDCG@10: 0.0890\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 75/500:\tLoss: 0.5373\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0286\tNDCG@10: 0.0825\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 76/500:\tLoss: 0.5336\tP@10: 0.0891\tR@10: 0.0445\tMAP@10: 0.0360\tNDCG@10: 0.0939\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 77/500:\tLoss: 0.5351\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0364\tNDCG@10: 0.0951\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 78/500:\tLoss: 0.5283\tP@10: 0.0861\tR@10: 0.0430\tMAP@10: 0.0351\tNDCG@10: 0.0922\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 79/500:\tLoss: 0.5337\tP@10: 0.0908\tR@10: 0.0454\tMAP@10: 0.0357\tNDCG@10: 0.0948\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 80/500:\tLoss: 0.5233\tP@10: 0.0938\tR@10: 0.0469\tMAP@10: 0.0361\tNDCG@10: 0.0971\tHR@10: 0.6092\n",
      "INFO:\ttrainer:\tEpoch 81/500:\tLoss: 0.5289\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0350\tNDCG@10: 0.0936\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 82/500:\tLoss: 0.5316\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0344\tNDCG@10: 0.0914\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 83/500:\tLoss: 0.5267\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0331\tNDCG@10: 0.0889\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 84/500:\tLoss: 0.5293\tP@10: 0.0798\tR@10: 0.0399\tMAP@10: 0.0318\tNDCG@10: 0.0854\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 85/500:\tLoss: 0.5245\tP@10: 0.0801\tR@10: 0.0401\tMAP@10: 0.0314\tNDCG@10: 0.0846\tHR@10: 0.5493\n",
      "INFO:\ttrainer:\tEpoch 86/500:\tLoss: 0.5232\tP@10: 0.0912\tR@10: 0.0456\tMAP@10: 0.0347\tNDCG@10: 0.0933\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 87/500:\tLoss: 0.5266\tP@10: 0.0859\tR@10: 0.0430\tMAP@10: 0.0329\tNDCG@10: 0.0898\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 88/500:\tLoss: 0.5264\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0332\tNDCG@10: 0.0901\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 89/500:\tLoss: 0.5267\tP@10: 0.0796\tR@10: 0.0398\tMAP@10: 0.0318\tNDCG@10: 0.0850\tHR@10: 0.5352\n",
      "INFO:\ttrainer:\tEpoch 90/500:\tLoss: 0.5243\tP@10: 0.0840\tR@10: 0.0420\tMAP@10: 0.0326\tNDCG@10: 0.0881\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 91/500:\tLoss: 0.5283\tP@10: 0.0924\tR@10: 0.0462\tMAP@10: 0.0353\tNDCG@10: 0.0948\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 92/500:\tLoss: 0.5218\tP@10: 0.0931\tR@10: 0.0466\tMAP@10: 0.0352\tNDCG@10: 0.0946\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 93/500:\tLoss: 0.5256\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0342\tNDCG@10: 0.0921\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 94/500:\tLoss: 0.5238\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0341\tNDCG@10: 0.0918\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 95/500:\tLoss: 0.5281\tP@10: 0.0785\tR@10: 0.0393\tMAP@10: 0.0314\tNDCG@10: 0.0846\tHR@10: 0.5370\n",
      "INFO:\ttrainer:\tEpoch 96/500:\tLoss: 0.5242\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0346\tNDCG@10: 0.0929\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 97/500:\tLoss: 0.5272\tP@10: 0.0847\tR@10: 0.0423\tMAP@10: 0.0329\tNDCG@10: 0.0888\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 98/500:\tLoss: 0.5247\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0339\tNDCG@10: 0.0909\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 99/500:\tLoss: 0.5182\tP@10: 0.0863\tR@10: 0.0431\tMAP@10: 0.0324\tNDCG@10: 0.0882\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 100/500:\tLoss: 0.5221\tP@10: 0.0877\tR@10: 0.0438\tMAP@10: 0.0337\tNDCG@10: 0.0914\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 101/500:\tLoss: 0.5244\tP@10: 0.0861\tR@10: 0.0430\tMAP@10: 0.0324\tNDCG@10: 0.0892\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 102/500:\tLoss: 0.5170\tP@10: 0.0833\tR@10: 0.0416\tMAP@10: 0.0316\tNDCG@10: 0.0865\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 103/500:\tLoss: 0.5185\tP@10: 0.0829\tR@10: 0.0415\tMAP@10: 0.0314\tNDCG@10: 0.0856\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 104/500:\tLoss: 0.5151\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0339\tNDCG@10: 0.0907\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 105/500:\tLoss: 0.5196\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0344\tNDCG@10: 0.0911\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 106/500:\tLoss: 0.5162\tP@10: 0.0912\tR@10: 0.0456\tMAP@10: 0.0341\tNDCG@10: 0.0925\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 107/500:\tLoss: 0.5172\tP@10: 0.0910\tR@10: 0.0455\tMAP@10: 0.0328\tNDCG@10: 0.0907\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 108/500:\tLoss: 0.5082\tP@10: 0.0926\tR@10: 0.0463\tMAP@10: 0.0340\tNDCG@10: 0.0928\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 109/500:\tLoss: 0.5106\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0338\tNDCG@10: 0.0924\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 110/500:\tLoss: 0.5141\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0312\tNDCG@10: 0.0872\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 111/500:\tLoss: 0.5201\tP@10: 0.0847\tR@10: 0.0423\tMAP@10: 0.0317\tNDCG@10: 0.0871\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 112/500:\tLoss: 0.5080\tP@10: 0.0933\tR@10: 0.0467\tMAP@10: 0.0344\tNDCG@10: 0.0939\tHR@10: 0.6039\n",
      "INFO:\ttrainer:\tEpoch 113/500:\tLoss: 0.5113\tP@10: 0.0798\tR@10: 0.0399\tMAP@10: 0.0319\tNDCG@10: 0.0851\tHR@10: 0.5370\n",
      "INFO:\ttrainer:\tEpoch 114/500:\tLoss: 0.5131\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0328\tNDCG@10: 0.0883\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 115/500:\tLoss: 0.5088\tP@10: 0.0827\tR@10: 0.0414\tMAP@10: 0.0317\tNDCG@10: 0.0860\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 116/500:\tLoss: 0.5100\tP@10: 0.0813\tR@10: 0.0407\tMAP@10: 0.0307\tNDCG@10: 0.0840\tHR@10: 0.5493\n",
      "INFO:\ttrainer:\tEpoch 117/500:\tLoss: 0.5154\tP@10: 0.0803\tR@10: 0.0401\tMAP@10: 0.0309\tNDCG@10: 0.0836\tHR@10: 0.5387\n",
      "INFO:\ttrainer:\tEpoch 118/500:\tLoss: 0.5186\tP@10: 0.0780\tR@10: 0.0390\tMAP@10: 0.0307\tNDCG@10: 0.0831\tHR@10: 0.5352\n",
      "INFO:\ttrainer:\tEpoch 119/500:\tLoss: 0.5122\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0313\tNDCG@10: 0.0852\tHR@10: 0.5546\n",
      "INFO:\ttrainer:\tEpoch 120/500:\tLoss: 0.5094\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0328\tNDCG@10: 0.0889\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 121/500:\tLoss: 0.5105\tP@10: 0.0850\tR@10: 0.0425\tMAP@10: 0.0318\tNDCG@10: 0.0872\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 122/500:\tLoss: 0.5154\tP@10: 0.0843\tR@10: 0.0422\tMAP@10: 0.0300\tNDCG@10: 0.0843\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 122\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run1/model.pth\n",
      "INFO:\tmain:\tCompleted run 1\n",
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 2,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run2/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run2\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 2 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_layer): CrossAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 1.0128\tP@10: 0.0827\tR@10: 0.0414\tMAP@10: 0.0334\tNDCG@10: 0.0879\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 1.0201\tP@10: 0.0371\tR@10: 0.0186\tMAP@10: 0.0139\tNDCG@10: 0.0407\tHR@10: 0.3187\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 1.1874\tP@10: 0.0634\tR@10: 0.0317\tMAP@10: 0.0237\tNDCG@10: 0.0661\tHR@10: 0.4577\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 1.1366\tP@10: 0.0607\tR@10: 0.0304\tMAP@10: 0.0201\tNDCG@10: 0.0604\tHR@10: 0.4630\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 1.0416\tP@10: 0.0579\tR@10: 0.0290\tMAP@10: 0.0167\tNDCG@10: 0.0533\tHR@10: 0.4401\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 0.9587\tP@10: 0.0644\tR@10: 0.0322\tMAP@10: 0.0182\tNDCG@10: 0.0585\tHR@10: 0.4824\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 0.8771\tP@10: 0.0826\tR@10: 0.0413\tMAP@10: 0.0343\tNDCG@10: 0.0908\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 0.8632\tP@10: 0.0741\tR@10: 0.0371\tMAP@10: 0.0288\tNDCG@10: 0.0783\tHR@10: 0.5317\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 0.8523\tP@10: 0.0857\tR@10: 0.0429\tMAP@10: 0.0320\tNDCG@10: 0.0881\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 0.8287\tP@10: 0.0819\tR@10: 0.0409\tMAP@10: 0.0312\tNDCG@10: 0.0851\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 0.8045\tP@10: 0.0871\tR@10: 0.0436\tMAP@10: 0.0305\tNDCG@10: 0.0861\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 12/500:\tLoss: 0.8022\tP@10: 0.0882\tR@10: 0.0441\tMAP@10: 0.0342\tNDCG@10: 0.0914\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 13/500:\tLoss: 0.7817\tP@10: 0.0944\tR@10: 0.0472\tMAP@10: 0.0382\tNDCG@10: 0.1009\tHR@10: 0.6268\n",
      "INFO:\ttrainer:\tEpoch 14/500:\tLoss: 0.7714\tP@10: 0.0792\tR@10: 0.0396\tMAP@10: 0.0310\tNDCG@10: 0.0830\tHR@10: 0.5405\n",
      "INFO:\ttrainer:\tEpoch 15/500:\tLoss: 0.7513\tP@10: 0.0739\tR@10: 0.0370\tMAP@10: 0.0271\tNDCG@10: 0.0750\tHR@10: 0.5141\n",
      "INFO:\ttrainer:\tEpoch 16/500:\tLoss: 0.7402\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0342\tNDCG@10: 0.0928\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 17/500:\tLoss: 0.7283\tP@10: 0.0799\tR@10: 0.0400\tMAP@10: 0.0273\tNDCG@10: 0.0793\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 18/500:\tLoss: 0.7134\tP@10: 0.0917\tR@10: 0.0459\tMAP@10: 0.0344\tNDCG@10: 0.0937\tHR@10: 0.6092\n",
      "INFO:\ttrainer:\tEpoch 19/500:\tLoss: 0.7022\tP@10: 0.0926\tR@10: 0.0463\tMAP@10: 0.0377\tNDCG@10: 0.0986\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 20/500:\tLoss: 0.6870\tP@10: 0.0801\tR@10: 0.0401\tMAP@10: 0.0312\tNDCG@10: 0.0841\tHR@10: 0.5352\n",
      "INFO:\ttrainer:\tEpoch 21/500:\tLoss: 0.6782\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0333\tNDCG@10: 0.0905\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 22/500:\tLoss: 0.6741\tP@10: 0.0956\tR@10: 0.0478\tMAP@10: 0.0380\tNDCG@10: 0.1005\tHR@10: 0.6162\n",
      "INFO:\ttrainer:\tEpoch 23/500:\tLoss: 0.6640\tP@10: 0.1000\tR@10: 0.0500\tMAP@10: 0.0395\tNDCG@10: 0.1045\tHR@10: 0.6250\n",
      "INFO:\ttrainer:\tEpoch 24/500:\tLoss: 0.6535\tP@10: 0.0954\tR@10: 0.0477\tMAP@10: 0.0383\tNDCG@10: 0.1010\tHR@10: 0.6109\n",
      "INFO:\ttrainer:\tEpoch 25/500:\tLoss: 0.6528\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0347\tNDCG@10: 0.0935\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 26/500:\tLoss: 0.6482\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0336\tNDCG@10: 0.0909\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 27/500:\tLoss: 0.6359\tP@10: 0.0859\tR@10: 0.0430\tMAP@10: 0.0341\tNDCG@10: 0.0913\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 28/500:\tLoss: 0.6346\tP@10: 0.0968\tR@10: 0.0484\tMAP@10: 0.0387\tNDCG@10: 0.1020\tHR@10: 0.6092\n",
      "INFO:\ttrainer:\tEpoch 29/500:\tLoss: 0.6313\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0348\tNDCG@10: 0.0938\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 30/500:\tLoss: 0.6218\tP@10: 0.0951\tR@10: 0.0475\tMAP@10: 0.0373\tNDCG@10: 0.0993\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 31/500:\tLoss: 0.6170\tP@10: 0.0926\tR@10: 0.0463\tMAP@10: 0.0354\tNDCG@10: 0.0951\tHR@10: 0.6092\n",
      "INFO:\ttrainer:\tEpoch 32/500:\tLoss: 0.6197\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0303\tNDCG@10: 0.0844\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 33/500:\tLoss: 0.6076\tP@10: 0.0942\tR@10: 0.0471\tMAP@10: 0.0347\tNDCG@10: 0.0949\tHR@10: 0.6039\n",
      "INFO:\ttrainer:\tEpoch 34/500:\tLoss: 0.6003\tP@10: 0.0926\tR@10: 0.0463\tMAP@10: 0.0342\tNDCG@10: 0.0940\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 35/500:\tLoss: 0.6098\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0325\tNDCG@10: 0.0877\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 36/500:\tLoss: 0.5962\tP@10: 0.0877\tR@10: 0.0438\tMAP@10: 0.0344\tNDCG@10: 0.0921\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 37/500:\tLoss: 0.6059\tP@10: 0.0827\tR@10: 0.0414\tMAP@10: 0.0313\tNDCG@10: 0.0864\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 38/500:\tLoss: 0.5922\tP@10: 0.0907\tR@10: 0.0453\tMAP@10: 0.0360\tNDCG@10: 0.0949\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 39/500:\tLoss: 0.5984\tP@10: 0.0850\tR@10: 0.0425\tMAP@10: 0.0346\tNDCG@10: 0.0900\tHR@10: 0.5546\n",
      "INFO:\ttrainer:\tEpoch 40/500:\tLoss: 0.5920\tP@10: 0.0947\tR@10: 0.0474\tMAP@10: 0.0339\tNDCG@10: 0.0939\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 41/500:\tLoss: 0.5919\tP@10: 0.0861\tR@10: 0.0430\tMAP@10: 0.0327\tNDCG@10: 0.0883\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 42/500:\tLoss: 0.5940\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0321\tNDCG@10: 0.0870\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 43/500:\tLoss: 0.5842\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0341\tNDCG@10: 0.0922\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 44/500:\tLoss: 0.5818\tP@10: 0.0857\tR@10: 0.0429\tMAP@10: 0.0332\tNDCG@10: 0.0888\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 45/500:\tLoss: 0.5818\tP@10: 0.0796\tR@10: 0.0398\tMAP@10: 0.0296\tNDCG@10: 0.0812\tHR@10: 0.5387\n",
      "INFO:\ttrainer:\tEpoch 46/500:\tLoss: 0.5758\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0319\tNDCG@10: 0.0882\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 47/500:\tLoss: 0.5703\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0353\tNDCG@10: 0.0946\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 48/500:\tLoss: 0.5681\tP@10: 0.0907\tR@10: 0.0453\tMAP@10: 0.0337\tNDCG@10: 0.0923\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 49/500:\tLoss: 0.5693\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0325\tNDCG@10: 0.0884\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 50/500:\tLoss: 0.5649\tP@10: 0.0937\tR@10: 0.0468\tMAP@10: 0.0324\tNDCG@10: 0.0917\tHR@10: 0.6127\n",
      "INFO:\ttrainer:\tEpoch 51/500:\tLoss: 0.5677\tP@10: 0.0919\tR@10: 0.0460\tMAP@10: 0.0337\tNDCG@10: 0.0923\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 52/500:\tLoss: 0.5681\tP@10: 0.0778\tR@10: 0.0389\tMAP@10: 0.0262\tNDCG@10: 0.0762\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 53/500:\tLoss: 0.5621\tP@10: 0.0796\tR@10: 0.0398\tMAP@10: 0.0261\tNDCG@10: 0.0767\tHR@10: 0.5511\n",
      "INFO:\ttrainer:\tEpoch 54/500:\tLoss: 0.5572\tP@10: 0.0857\tR@10: 0.0429\tMAP@10: 0.0335\tNDCG@10: 0.0901\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 55/500:\tLoss: 0.5589\tP@10: 0.0917\tR@10: 0.0459\tMAP@10: 0.0362\tNDCG@10: 0.0969\tHR@10: 0.6162\n",
      "INFO:\ttrainer:\tEpoch 56/500:\tLoss: 0.5618\tP@10: 0.0820\tR@10: 0.0410\tMAP@10: 0.0326\tNDCG@10: 0.0874\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 57/500:\tLoss: 0.5628\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0325\tNDCG@10: 0.0877\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 58/500:\tLoss: 0.5641\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0325\tNDCG@10: 0.0900\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 59/500:\tLoss: 0.5522\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0342\tNDCG@10: 0.0926\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 60/500:\tLoss: 0.5534\tP@10: 0.0935\tR@10: 0.0467\tMAP@10: 0.0343\tNDCG@10: 0.0939\tHR@10: 0.6039\n",
      "INFO:\ttrainer:\tEpoch 61/500:\tLoss: 0.5553\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0354\tNDCG@10: 0.0942\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 62/500:\tLoss: 0.5529\tP@10: 0.0942\tR@10: 0.0471\tMAP@10: 0.0350\tNDCG@10: 0.0952\tHR@10: 0.6197\n",
      "INFO:\ttrainer:\tEpoch 63/500:\tLoss: 0.5503\tP@10: 0.0842\tR@10: 0.0421\tMAP@10: 0.0327\tNDCG@10: 0.0877\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 64/500:\tLoss: 0.5549\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0336\tNDCG@10: 0.0900\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 65/500:\tLoss: 0.5493\tP@10: 0.0931\tR@10: 0.0466\tMAP@10: 0.0352\tNDCG@10: 0.0952\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 66/500:\tLoss: 0.5450\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0336\tNDCG@10: 0.0909\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 67/500:\tLoss: 0.5469\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0335\tNDCG@10: 0.0904\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 68/500:\tLoss: 0.5456\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0296\tNDCG@10: 0.0851\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 69/500:\tLoss: 0.5466\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0347\tNDCG@10: 0.0941\tHR@10: 0.6092\n",
      "INFO:\ttrainer:\tEpoch 70/500:\tLoss: 0.5438\tP@10: 0.0813\tR@10: 0.0407\tMAP@10: 0.0315\tNDCG@10: 0.0860\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 71/500:\tLoss: 0.5454\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0338\tNDCG@10: 0.0908\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 72/500:\tLoss: 0.5402\tP@10: 0.0810\tR@10: 0.0405\tMAP@10: 0.0303\tNDCG@10: 0.0832\tHR@10: 0.5511\n",
      "INFO:\ttrainer:\tEpoch 73/500:\tLoss: 0.5422\tP@10: 0.0868\tR@10: 0.0434\tMAP@10: 0.0318\tNDCG@10: 0.0884\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 74/500:\tLoss: 0.5406\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0321\tNDCG@10: 0.0880\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 75/500:\tLoss: 0.5366\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0334\tNDCG@10: 0.0912\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 76/500:\tLoss: 0.5386\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0321\tNDCG@10: 0.0876\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 77/500:\tLoss: 0.5318\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0343\tNDCG@10: 0.0919\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 78/500:\tLoss: 0.5371\tP@10: 0.0921\tR@10: 0.0460\tMAP@10: 0.0359\tNDCG@10: 0.0948\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 79/500:\tLoss: 0.5311\tP@10: 0.0908\tR@10: 0.0454\tMAP@10: 0.0357\tNDCG@10: 0.0949\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 80/500:\tLoss: 0.5312\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0346\tNDCG@10: 0.0921\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 81/500:\tLoss: 0.5295\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0350\tNDCG@10: 0.0919\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 82/500:\tLoss: 0.5335\tP@10: 0.0827\tR@10: 0.0414\tMAP@10: 0.0317\tNDCG@10: 0.0854\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 83/500:\tLoss: 0.5314\tP@10: 0.0850\tR@10: 0.0425\tMAP@10: 0.0334\tNDCG@10: 0.0883\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 84/500:\tLoss: 0.5323\tP@10: 0.0919\tR@10: 0.0460\tMAP@10: 0.0355\tNDCG@10: 0.0944\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 85/500:\tLoss: 0.5294\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0360\tNDCG@10: 0.0949\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 86/500:\tLoss: 0.5308\tP@10: 0.0857\tR@10: 0.0429\tMAP@10: 0.0350\tNDCG@10: 0.0919\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 87/500:\tLoss: 0.5308\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0328\tNDCG@10: 0.0886\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 88/500:\tLoss: 0.5356\tP@10: 0.0838\tR@10: 0.0419\tMAP@10: 0.0322\tNDCG@10: 0.0872\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 89/500:\tLoss: 0.5303\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0348\tNDCG@10: 0.0930\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 90/500:\tLoss: 0.5230\tP@10: 0.0924\tR@10: 0.0462\tMAP@10: 0.0350\tNDCG@10: 0.0939\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 91/500:\tLoss: 0.5288\tP@10: 0.0982\tR@10: 0.0491\tMAP@10: 0.0367\tNDCG@10: 0.0982\tHR@10: 0.6109\n",
      "INFO:\ttrainer:\tEpoch 92/500:\tLoss: 0.5347\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0344\tNDCG@10: 0.0910\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 93/500:\tLoss: 0.5230\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0337\tNDCG@10: 0.0912\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 94/500:\tLoss: 0.5249\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0351\tNDCG@10: 0.0939\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 95/500:\tLoss: 0.5326\tP@10: 0.0810\tR@10: 0.0405\tMAP@10: 0.0328\tNDCG@10: 0.0883\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 96/500:\tLoss: 0.5298\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0340\tNDCG@10: 0.0931\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 97/500:\tLoss: 0.5181\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0338\tNDCG@10: 0.0903\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 98/500:\tLoss: 0.5262\tP@10: 0.0843\tR@10: 0.0422\tMAP@10: 0.0313\tNDCG@10: 0.0856\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 99/500:\tLoss: 0.5202\tP@10: 0.0912\tR@10: 0.0456\tMAP@10: 0.0294\tNDCG@10: 0.0847\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 100/500:\tLoss: 0.5280\tP@10: 0.0827\tR@10: 0.0414\tMAP@10: 0.0319\tNDCG@10: 0.0861\tHR@10: 0.5546\n",
      "INFO:\ttrainer:\tEpoch 101/500:\tLoss: 0.5231\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0342\tNDCG@10: 0.0909\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 102/500:\tLoss: 0.5219\tP@10: 0.0945\tR@10: 0.0473\tMAP@10: 0.0358\tNDCG@10: 0.0963\tHR@10: 0.6074\n",
      "INFO:\ttrainer:\tEpoch 103/500:\tLoss: 0.5187\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0334\tNDCG@10: 0.0910\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 104/500:\tLoss: 0.5188\tP@10: 0.0935\tR@10: 0.0467\tMAP@10: 0.0326\tNDCG@10: 0.0916\tHR@10: 0.6109\n",
      "INFO:\ttrainer:\tEpoch 105/500:\tLoss: 0.5203\tP@10: 0.0850\tR@10: 0.0425\tMAP@10: 0.0310\tNDCG@10: 0.0863\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 106/500:\tLoss: 0.5210\tP@10: 0.0930\tR@10: 0.0465\tMAP@10: 0.0351\tNDCG@10: 0.0944\tHR@10: 0.6056\n",
      "INFO:\ttrainer:\tEpoch 107/500:\tLoss: 0.5229\tP@10: 0.0949\tR@10: 0.0474\tMAP@10: 0.0361\tNDCG@10: 0.0969\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 107\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run2/model.pth\n",
      "INFO:\tmain:\tCompleted run 2\n",
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 3,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run3/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run3\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 3 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 2.6013\tP@10: 0.0822\tR@10: 0.0411\tMAP@10: 0.0313\tNDCG@10: 0.0857\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 3.1582\tP@10: 0.0539\tR@10: 0.0269\tMAP@10: 0.0236\tNDCG@10: 0.0622\tHR@10: 0.4155\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 3.4650\tP@10: 0.0521\tR@10: 0.0261\tMAP@10: 0.0215\tNDCG@10: 0.0589\tHR@10: 0.4173\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 3.2209\tP@10: 0.0447\tR@10: 0.0224\tMAP@10: 0.0115\tNDCG@10: 0.0402\tHR@10: 0.3891\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 3.4374\tP@10: 0.0553\tR@10: 0.0276\tMAP@10: 0.0178\tNDCG@10: 0.0534\tHR@10: 0.4225\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 3.4869\tP@10: 0.0565\tR@10: 0.0283\tMAP@10: 0.0160\tNDCG@10: 0.0515\tHR@10: 0.4384\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 3.3789\tP@10: 0.0437\tR@10: 0.0218\tMAP@10: 0.0136\tNDCG@10: 0.0433\tHR@10: 0.3750\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 3.2661\tP@10: 0.0437\tR@10: 0.0218\tMAP@10: 0.0142\tNDCG@10: 0.0442\tHR@10: 0.3644\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 3.2320\tP@10: 0.0523\tR@10: 0.0261\tMAP@10: 0.0188\tNDCG@10: 0.0551\tHR@10: 0.4173\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 3.1189\tP@10: 0.0493\tR@10: 0.0246\tMAP@10: 0.0153\tNDCG@10: 0.0484\tHR@10: 0.4049\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 3.0944\tP@10: 0.0419\tR@10: 0.0210\tMAP@10: 0.0143\tNDCG@10: 0.0436\tHR@10: 0.3556\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 11\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run3/model.pth\n",
      "INFO:\tmain:\tCompleted run 3\n",
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 4,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run4/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run4\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 4 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 2.5353\tP@10: 0.0651\tR@10: 0.0326\tMAP@10: 0.0243\tNDCG@10: 0.0687\tHR@10: 0.4806\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 3.2771\tP@10: 0.0391\tR@10: 0.0195\tMAP@10: 0.0143\tNDCG@10: 0.0415\tHR@10: 0.3204\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 3.4861\tP@10: 0.0440\tR@10: 0.0220\tMAP@10: 0.0178\tNDCG@10: 0.0499\tHR@10: 0.3574\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 3.2133\tP@10: 0.0387\tR@10: 0.0194\tMAP@10: 0.0127\tNDCG@10: 0.0391\tHR@10: 0.3275\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 3.4920\tP@10: 0.0336\tR@10: 0.0168\tMAP@10: 0.0071\tNDCG@10: 0.0278\tHR@10: 0.2958\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 3.3143\tP@10: 0.0512\tR@10: 0.0256\tMAP@10: 0.0150\tNDCG@10: 0.0483\tHR@10: 0.4085\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 3.3960\tP@10: 0.0514\tR@10: 0.0257\tMAP@10: 0.0138\tNDCG@10: 0.0459\tHR@10: 0.4032\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 3.3707\tP@10: 0.0514\tR@10: 0.0257\tMAP@10: 0.0139\tNDCG@10: 0.0466\tHR@10: 0.4120\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 3.1089\tP@10: 0.0526\tR@10: 0.0263\tMAP@10: 0.0205\tNDCG@10: 0.0580\tHR@10: 0.4261\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 3.1709\tP@10: 0.0544\tR@10: 0.0272\tMAP@10: 0.0226\tNDCG@10: 0.0617\tHR@10: 0.4190\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 2.8859\tP@10: 0.0734\tR@10: 0.0367\tMAP@10: 0.0290\tNDCG@10: 0.0796\tHR@10: 0.5493\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 11\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run4/model.pth\n",
      "INFO:\tmain:\tCompleted run 4\n"
     ]
    }
   ],
   "source": [
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed/ml-100k/ --model_variant 1\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed/ml-100k/ --model_variant 2\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed/ml-100k/ --model_variant 3\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed/ml-100k/ --model_variant 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 1,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run5/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run5\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 1 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_layer): CrossAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 0.9968\tP@10: 0.0785\tR@10: 0.0393\tMAP@10: 0.0327\tNDCG@10: 0.0859\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 0.9890\tP@10: 0.0407\tR@10: 0.0203\tMAP@10: 0.0133\tNDCG@10: 0.0410\tHR@10: 0.3415\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 1.0815\tP@10: 0.0470\tR@10: 0.0235\tMAP@10: 0.0128\tNDCG@10: 0.0426\tHR@10: 0.3768\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 1.0482\tP@10: 0.0523\tR@10: 0.0261\tMAP@10: 0.0211\tNDCG@10: 0.0587\tHR@10: 0.4173\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 0.9441\tP@10: 0.0600\tR@10: 0.0300\tMAP@10: 0.0177\tNDCG@10: 0.0557\tHR@10: 0.4613\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 0.9741\tP@10: 0.0826\tR@10: 0.0413\tMAP@10: 0.0312\tNDCG@10: 0.0848\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 0.9388\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0336\tNDCG@10: 0.0918\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 0.8458\tP@10: 0.0762\tR@10: 0.0381\tMAP@10: 0.0243\tNDCG@10: 0.0717\tHR@10: 0.5211\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 0.8239\tP@10: 0.0891\tR@10: 0.0445\tMAP@10: 0.0362\tNDCG@10: 0.0940\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 0.8181\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0343\tNDCG@10: 0.0921\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 0.7903\tP@10: 0.0991\tR@10: 0.0496\tMAP@10: 0.0370\tNDCG@10: 0.0996\tHR@10: 0.6197\n",
      "INFO:\ttrainer:\tEpoch 12/500:\tLoss: 0.7767\tP@10: 0.0970\tR@10: 0.0485\tMAP@10: 0.0374\tNDCG@10: 0.1006\tHR@10: 0.6285\n",
      "INFO:\ttrainer:\tEpoch 13/500:\tLoss: 0.7642\tP@10: 0.0745\tR@10: 0.0372\tMAP@10: 0.0305\tNDCG@10: 0.0816\tHR@10: 0.5317\n",
      "INFO:\ttrainer:\tEpoch 14/500:\tLoss: 0.7602\tP@10: 0.0849\tR@10: 0.0424\tMAP@10: 0.0272\tNDCG@10: 0.0792\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 15/500:\tLoss: 0.7421\tP@10: 0.0977\tR@10: 0.0489\tMAP@10: 0.0335\tNDCG@10: 0.0939\tHR@10: 0.6285\n",
      "INFO:\ttrainer:\tEpoch 16/500:\tLoss: 0.7259\tP@10: 0.0954\tR@10: 0.0477\tMAP@10: 0.0391\tNDCG@10: 0.1024\tHR@10: 0.6338\n",
      "INFO:\ttrainer:\tEpoch 17/500:\tLoss: 0.7168\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0373\tNDCG@10: 0.0973\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 18/500:\tLoss: 0.7019\tP@10: 0.0801\tR@10: 0.0401\tMAP@10: 0.0272\tNDCG@10: 0.0781\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 19/500:\tLoss: 0.6869\tP@10: 0.0826\tR@10: 0.0413\tMAP@10: 0.0316\tNDCG@10: 0.0857\tHR@10: 0.5493\n",
      "INFO:\ttrainer:\tEpoch 20/500:\tLoss: 0.6859\tP@10: 0.0833\tR@10: 0.0416\tMAP@10: 0.0324\tNDCG@10: 0.0879\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 21/500:\tLoss: 0.6675\tP@10: 0.0882\tR@10: 0.0441\tMAP@10: 0.0311\tNDCG@10: 0.0881\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 22/500:\tLoss: 0.6611\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0333\tNDCG@10: 0.0903\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 23/500:\tLoss: 0.6536\tP@10: 0.0826\tR@10: 0.0413\tMAP@10: 0.0319\tNDCG@10: 0.0869\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 24/500:\tLoss: 0.6516\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0311\tNDCG@10: 0.0894\tHR@10: 0.6162\n",
      "INFO:\ttrainer:\tEpoch 25/500:\tLoss: 0.6472\tP@10: 0.0958\tR@10: 0.0479\tMAP@10: 0.0323\tNDCG@10: 0.0912\tHR@10: 0.6197\n",
      "INFO:\ttrainer:\tEpoch 26/500:\tLoss: 0.6369\tP@10: 0.0951\tR@10: 0.0475\tMAP@10: 0.0367\tNDCG@10: 0.0973\tHR@10: 0.6074\n",
      "INFO:\ttrainer:\tEpoch 27/500:\tLoss: 0.6374\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0349\tNDCG@10: 0.0940\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 28/500:\tLoss: 0.6211\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0342\tNDCG@10: 0.0912\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 29/500:\tLoss: 0.6238\tP@10: 0.0824\tR@10: 0.0412\tMAP@10: 0.0312\tNDCG@10: 0.0853\tHR@10: 0.5475\n",
      "INFO:\ttrainer:\tEpoch 30/500:\tLoss: 0.6149\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0337\tNDCG@10: 0.0915\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 31/500:\tLoss: 0.6103\tP@10: 0.0903\tR@10: 0.0452\tMAP@10: 0.0297\tNDCG@10: 0.0857\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 32/500:\tLoss: 0.6103\tP@10: 0.0835\tR@10: 0.0417\tMAP@10: 0.0286\tNDCG@10: 0.0806\tHR@10: 0.5511\n",
      "INFO:\ttrainer:\tEpoch 33/500:\tLoss: 0.5946\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0309\tNDCG@10: 0.0848\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 34/500:\tLoss: 0.6062\tP@10: 0.0842\tR@10: 0.0421\tMAP@10: 0.0311\tNDCG@10: 0.0859\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 35/500:\tLoss: 0.5988\tP@10: 0.0813\tR@10: 0.0407\tMAP@10: 0.0282\tNDCG@10: 0.0806\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 36/500:\tLoss: 0.5963\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0340\tNDCG@10: 0.0921\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 37/500:\tLoss: 0.5892\tP@10: 0.0891\tR@10: 0.0445\tMAP@10: 0.0342\tNDCG@10: 0.0922\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 38/500:\tLoss: 0.5942\tP@10: 0.0822\tR@10: 0.0411\tMAP@10: 0.0328\tNDCG@10: 0.0879\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 39/500:\tLoss: 0.5873\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0330\tNDCG@10: 0.0894\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 40/500:\tLoss: 0.5789\tP@10: 0.0861\tR@10: 0.0430\tMAP@10: 0.0353\tNDCG@10: 0.0928\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 41/500:\tLoss: 0.5815\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0356\tNDCG@10: 0.0955\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 42/500:\tLoss: 0.5776\tP@10: 0.0827\tR@10: 0.0414\tMAP@10: 0.0318\tNDCG@10: 0.0863\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 43/500:\tLoss: 0.5768\tP@10: 0.0805\tR@10: 0.0402\tMAP@10: 0.0323\tNDCG@10: 0.0868\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 44/500:\tLoss: 0.5743\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0338\tNDCG@10: 0.0933\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 45/500:\tLoss: 0.5739\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0328\tNDCG@10: 0.0891\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 46/500:\tLoss: 0.5710\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0334\tNDCG@10: 0.0897\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 47/500:\tLoss: 0.5660\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0335\tNDCG@10: 0.0909\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 48/500:\tLoss: 0.5655\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0326\tNDCG@10: 0.0896\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 49/500:\tLoss: 0.5680\tP@10: 0.0861\tR@10: 0.0430\tMAP@10: 0.0315\tNDCG@10: 0.0865\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 50/500:\tLoss: 0.5624\tP@10: 0.0908\tR@10: 0.0454\tMAP@10: 0.0323\tNDCG@10: 0.0901\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 51/500:\tLoss: 0.5581\tP@10: 0.0850\tR@10: 0.0425\tMAP@10: 0.0315\tNDCG@10: 0.0867\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 52/500:\tLoss: 0.5625\tP@10: 0.0835\tR@10: 0.0417\tMAP@10: 0.0328\tNDCG@10: 0.0872\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 53/500:\tLoss: 0.5620\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0340\tNDCG@10: 0.0920\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 54/500:\tLoss: 0.5646\tP@10: 0.0819\tR@10: 0.0409\tMAP@10: 0.0331\tNDCG@10: 0.0885\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 55/500:\tLoss: 0.5531\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0320\tNDCG@10: 0.0880\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 56/500:\tLoss: 0.5580\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0341\tNDCG@10: 0.0922\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 57/500:\tLoss: 0.5556\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0354\tNDCG@10: 0.0940\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 58/500:\tLoss: 0.5554\tP@10: 0.0849\tR@10: 0.0424\tMAP@10: 0.0324\tNDCG@10: 0.0878\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 59/500:\tLoss: 0.5516\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0338\tNDCG@10: 0.0907\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 60/500:\tLoss: 0.5489\tP@10: 0.0947\tR@10: 0.0474\tMAP@10: 0.0373\tNDCG@10: 0.0984\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 61/500:\tLoss: 0.5494\tP@10: 0.0820\tR@10: 0.0410\tMAP@10: 0.0323\tNDCG@10: 0.0866\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 62/500:\tLoss: 0.5428\tP@10: 0.0718\tR@10: 0.0359\tMAP@10: 0.0240\tNDCG@10: 0.0702\tHR@10: 0.5211\n",
      "INFO:\ttrainer:\tEpoch 63/500:\tLoss: 0.5442\tP@10: 0.0782\tR@10: 0.0391\tMAP@10: 0.0291\tNDCG@10: 0.0806\tHR@10: 0.5423\n",
      "INFO:\ttrainer:\tEpoch 64/500:\tLoss: 0.5454\tP@10: 0.0785\tR@10: 0.0393\tMAP@10: 0.0319\tNDCG@10: 0.0839\tHR@10: 0.5282\n",
      "INFO:\ttrainer:\tEpoch 65/500:\tLoss: 0.5480\tP@10: 0.0903\tR@10: 0.0452\tMAP@10: 0.0365\tNDCG@10: 0.0953\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 66/500:\tLoss: 0.5424\tP@10: 0.0919\tR@10: 0.0460\tMAP@10: 0.0352\tNDCG@10: 0.0945\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 67/500:\tLoss: 0.5394\tP@10: 0.0826\tR@10: 0.0413\tMAP@10: 0.0278\tNDCG@10: 0.0797\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 68/500:\tLoss: 0.5411\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0319\tNDCG@10: 0.0879\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 69/500:\tLoss: 0.5402\tP@10: 0.0921\tR@10: 0.0460\tMAP@10: 0.0349\tNDCG@10: 0.0937\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 70/500:\tLoss: 0.5402\tP@10: 0.0819\tR@10: 0.0409\tMAP@10: 0.0336\tNDCG@10: 0.0876\tHR@10: 0.5405\n",
      "INFO:\ttrainer:\tEpoch 71/500:\tLoss: 0.5371\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0341\tNDCG@10: 0.0901\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 72/500:\tLoss: 0.5412\tP@10: 0.0937\tR@10: 0.0468\tMAP@10: 0.0349\tNDCG@10: 0.0937\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 73/500:\tLoss: 0.5400\tP@10: 0.0923\tR@10: 0.0461\tMAP@10: 0.0349\tNDCG@10: 0.0935\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 74/500:\tLoss: 0.5373\tP@10: 0.0863\tR@10: 0.0431\tMAP@10: 0.0331\tNDCG@10: 0.0888\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 75/500:\tLoss: 0.5344\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0346\tNDCG@10: 0.0932\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 76/500:\tLoss: 0.5391\tP@10: 0.0944\tR@10: 0.0472\tMAP@10: 0.0359\tNDCG@10: 0.0966\tHR@10: 0.6039\n",
      "INFO:\ttrainer:\tEpoch 77/500:\tLoss: 0.5351\tP@10: 0.0940\tR@10: 0.0470\tMAP@10: 0.0366\tNDCG@10: 0.0969\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 78/500:\tLoss: 0.5379\tP@10: 0.0884\tR@10: 0.0442\tMAP@10: 0.0354\tNDCG@10: 0.0926\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 79/500:\tLoss: 0.5298\tP@10: 0.0863\tR@10: 0.0431\tMAP@10: 0.0339\tNDCG@10: 0.0899\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 80/500:\tLoss: 0.5283\tP@10: 0.0921\tR@10: 0.0460\tMAP@10: 0.0352\tNDCG@10: 0.0933\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 81/500:\tLoss: 0.5220\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0343\tNDCG@10: 0.0916\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 82/500:\tLoss: 0.5240\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0336\tNDCG@10: 0.0886\tHR@10: 0.5440\n",
      "INFO:\ttrainer:\tEpoch 83/500:\tLoss: 0.5312\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0357\tNDCG@10: 0.0941\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 84/500:\tLoss: 0.5230\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0334\tNDCG@10: 0.0906\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 85/500:\tLoss: 0.5298\tP@10: 0.0891\tR@10: 0.0445\tMAP@10: 0.0339\tNDCG@10: 0.0924\tHR@10: 0.6056\n",
      "INFO:\ttrainer:\tEpoch 86/500:\tLoss: 0.5284\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0350\tNDCG@10: 0.0933\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 87/500:\tLoss: 0.5255\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0351\tNDCG@10: 0.0937\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 88/500:\tLoss: 0.5258\tP@10: 0.0944\tR@10: 0.0472\tMAP@10: 0.0357\tNDCG@10: 0.0956\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 89/500:\tLoss: 0.5303\tP@10: 0.0940\tR@10: 0.0470\tMAP@10: 0.0358\tNDCG@10: 0.0955\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 90/500:\tLoss: 0.5221\tP@10: 0.0935\tR@10: 0.0467\tMAP@10: 0.0368\tNDCG@10: 0.0965\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 91/500:\tLoss: 0.5171\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0362\tNDCG@10: 0.0951\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 92/500:\tLoss: 0.5234\tP@10: 0.0842\tR@10: 0.0421\tMAP@10: 0.0331\tNDCG@10: 0.0883\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 93/500:\tLoss: 0.5253\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0339\tNDCG@10: 0.0899\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 94/500:\tLoss: 0.5186\tP@10: 0.0912\tR@10: 0.0456\tMAP@10: 0.0339\tNDCG@10: 0.0920\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 95/500:\tLoss: 0.5238\tP@10: 0.0746\tR@10: 0.0373\tMAP@10: 0.0283\tNDCG@10: 0.0778\tHR@10: 0.5264\n",
      "INFO:\ttrainer:\tEpoch 96/500:\tLoss: 0.5235\tP@10: 0.0840\tR@10: 0.0420\tMAP@10: 0.0330\tNDCG@10: 0.0880\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 97/500:\tLoss: 0.5174\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0344\tNDCG@10: 0.0921\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 98/500:\tLoss: 0.5271\tP@10: 0.0847\tR@10: 0.0423\tMAP@10: 0.0344\tNDCG@10: 0.0908\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 99/500:\tLoss: 0.5221\tP@10: 0.0799\tR@10: 0.0400\tMAP@10: 0.0322\tNDCG@10: 0.0857\tHR@10: 0.5475\n",
      "INFO:\ttrainer:\tEpoch 100/500:\tLoss: 0.5197\tP@10: 0.0813\tR@10: 0.0407\tMAP@10: 0.0316\tNDCG@10: 0.0861\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 101/500:\tLoss: 0.5201\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0356\tNDCG@10: 0.0939\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 101\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run5/model.pth\n",
      "INFO:\tmain:\tCompleted run 5\n",
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 2,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run6/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run6\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 2 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_layer): CrossAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 1.0108\tP@10: 0.0444\tR@10: 0.0222\tMAP@10: 0.0146\tNDCG@10: 0.0446\tHR@10: 0.3592\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 1.1067\tP@10: 0.0518\tR@10: 0.0259\tMAP@10: 0.0138\tNDCG@10: 0.0465\tHR@10: 0.4243\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 1.0567\tP@10: 0.0620\tR@10: 0.0310\tMAP@10: 0.0239\tNDCG@10: 0.0669\tHR@10: 0.4736\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 1.1153\tP@10: 0.0725\tR@10: 0.0363\tMAP@10: 0.0214\tNDCG@10: 0.0669\tHR@10: 0.5246\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 1.0023\tP@10: 0.0748\tR@10: 0.0374\tMAP@10: 0.0306\tNDCG@10: 0.0819\tHR@10: 0.5335\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 0.8973\tP@10: 0.0711\tR@10: 0.0356\tMAP@10: 0.0275\tNDCG@10: 0.0756\tHR@10: 0.5141\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 0.8847\tP@10: 0.0776\tR@10: 0.0388\tMAP@10: 0.0324\tNDCG@10: 0.0840\tHR@10: 0.5211\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 0.8322\tP@10: 0.0588\tR@10: 0.0294\tMAP@10: 0.0258\tNDCG@10: 0.0682\tHR@10: 0.4472\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 0.8304\tP@10: 0.0720\tR@10: 0.0360\tMAP@10: 0.0238\tNDCG@10: 0.0704\tHR@10: 0.5176\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 0.8203\tP@10: 0.0748\tR@10: 0.0374\tMAP@10: 0.0219\tNDCG@10: 0.0684\tHR@10: 0.5440\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 0.8023\tP@10: 0.0884\tR@10: 0.0442\tMAP@10: 0.0346\tNDCG@10: 0.0932\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 12/500:\tLoss: 0.7806\tP@10: 0.0820\tR@10: 0.0410\tMAP@10: 0.0340\tNDCG@10: 0.0892\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 13/500:\tLoss: 0.7792\tP@10: 0.0796\tR@10: 0.0398\tMAP@10: 0.0309\tNDCG@10: 0.0839\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 14/500:\tLoss: 0.7595\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0269\tNDCG@10: 0.0794\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 15/500:\tLoss: 0.7496\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0309\tNDCG@10: 0.0867\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 16/500:\tLoss: 0.7344\tP@10: 0.0931\tR@10: 0.0466\tMAP@10: 0.0368\tNDCG@10: 0.0973\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 17/500:\tLoss: 0.7238\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0373\tNDCG@10: 0.0981\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 18/500:\tLoss: 0.7132\tP@10: 0.0813\tR@10: 0.0407\tMAP@10: 0.0310\tNDCG@10: 0.0851\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 19/500:\tLoss: 0.6935\tP@10: 0.0835\tR@10: 0.0417\tMAP@10: 0.0300\tNDCG@10: 0.0841\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 20/500:\tLoss: 0.6864\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0347\tNDCG@10: 0.0925\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 21/500:\tLoss: 0.6812\tP@10: 0.0949\tR@10: 0.0474\tMAP@10: 0.0364\tNDCG@10: 0.0981\tHR@10: 0.6250\n",
      "INFO:\ttrainer:\tEpoch 22/500:\tLoss: 0.6681\tP@10: 0.0907\tR@10: 0.0453\tMAP@10: 0.0377\tNDCG@10: 0.0983\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 23/500:\tLoss: 0.6618\tP@10: 0.0859\tR@10: 0.0430\tMAP@10: 0.0357\tNDCG@10: 0.0941\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 24/500:\tLoss: 0.6526\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0371\tNDCG@10: 0.0966\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 25/500:\tLoss: 0.6518\tP@10: 0.0787\tR@10: 0.0393\tMAP@10: 0.0337\tNDCG@10: 0.0889\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 26/500:\tLoss: 0.6455\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0362\tNDCG@10: 0.0954\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 27/500:\tLoss: 0.6355\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0338\tNDCG@10: 0.0917\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 28/500:\tLoss: 0.6330\tP@10: 0.0796\tR@10: 0.0398\tMAP@10: 0.0339\tNDCG@10: 0.0880\tHR@10: 0.5440\n",
      "INFO:\ttrainer:\tEpoch 29/500:\tLoss: 0.6248\tP@10: 0.0829\tR@10: 0.0415\tMAP@10: 0.0334\tNDCG@10: 0.0882\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 30/500:\tLoss: 0.6253\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0339\tNDCG@10: 0.0906\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 31/500:\tLoss: 0.6219\tP@10: 0.0838\tR@10: 0.0419\tMAP@10: 0.0334\tNDCG@10: 0.0895\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 32/500:\tLoss: 0.6104\tP@10: 0.0958\tR@10: 0.0479\tMAP@10: 0.0370\tNDCG@10: 0.0993\tHR@10: 0.6268\n",
      "INFO:\ttrainer:\tEpoch 33/500:\tLoss: 0.6081\tP@10: 0.0938\tR@10: 0.0469\tMAP@10: 0.0370\tNDCG@10: 0.0981\tHR@10: 0.6127\n",
      "INFO:\ttrainer:\tEpoch 34/500:\tLoss: 0.5996\tP@10: 0.0935\tR@10: 0.0467\tMAP@10: 0.0369\tNDCG@10: 0.0978\tHR@10: 0.6144\n",
      "INFO:\ttrainer:\tEpoch 35/500:\tLoss: 0.6049\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0349\tNDCG@10: 0.0934\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 36/500:\tLoss: 0.5919\tP@10: 0.0813\tR@10: 0.0407\tMAP@10: 0.0338\tNDCG@10: 0.0891\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 37/500:\tLoss: 0.5961\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0343\tNDCG@10: 0.0920\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 38/500:\tLoss: 0.5859\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0345\tNDCG@10: 0.0927\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 39/500:\tLoss: 0.5848\tP@10: 0.0838\tR@10: 0.0419\tMAP@10: 0.0315\tNDCG@10: 0.0862\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 40/500:\tLoss: 0.5841\tP@10: 0.0799\tR@10: 0.0400\tMAP@10: 0.0272\tNDCG@10: 0.0776\tHR@10: 0.5387\n",
      "INFO:\ttrainer:\tEpoch 41/500:\tLoss: 0.5881\tP@10: 0.0833\tR@10: 0.0416\tMAP@10: 0.0263\tNDCG@10: 0.0780\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 42/500:\tLoss: 0.5787\tP@10: 0.0803\tR@10: 0.0401\tMAP@10: 0.0235\tNDCG@10: 0.0727\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 43/500:\tLoss: 0.5793\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0305\tNDCG@10: 0.0852\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 44/500:\tLoss: 0.5722\tP@10: 0.0817\tR@10: 0.0408\tMAP@10: 0.0308\tNDCG@10: 0.0846\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 45/500:\tLoss: 0.5766\tP@10: 0.0782\tR@10: 0.0391\tMAP@10: 0.0313\tNDCG@10: 0.0837\tHR@10: 0.5335\n",
      "INFO:\ttrainer:\tEpoch 46/500:\tLoss: 0.5703\tP@10: 0.0801\tR@10: 0.0401\tMAP@10: 0.0303\tNDCG@10: 0.0827\tHR@10: 0.5475\n",
      "INFO:\ttrainer:\tEpoch 47/500:\tLoss: 0.5676\tP@10: 0.0843\tR@10: 0.0422\tMAP@10: 0.0298\tNDCG@10: 0.0842\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 48/500:\tLoss: 0.5637\tP@10: 0.0868\tR@10: 0.0434\tMAP@10: 0.0316\tNDCG@10: 0.0877\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 49/500:\tLoss: 0.5611\tP@10: 0.0972\tR@10: 0.0486\tMAP@10: 0.0377\tNDCG@10: 0.1003\tHR@10: 0.6092\n",
      "INFO:\ttrainer:\tEpoch 50/500:\tLoss: 0.5617\tP@10: 0.0965\tR@10: 0.0482\tMAP@10: 0.0374\tNDCG@10: 0.0996\tHR@10: 0.6144\n",
      "INFO:\ttrainer:\tEpoch 51/500:\tLoss: 0.5565\tP@10: 0.0944\tR@10: 0.0472\tMAP@10: 0.0360\tNDCG@10: 0.0971\tHR@10: 0.6074\n",
      "INFO:\ttrainer:\tEpoch 52/500:\tLoss: 0.5585\tP@10: 0.0884\tR@10: 0.0442\tMAP@10: 0.0351\tNDCG@10: 0.0936\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 53/500:\tLoss: 0.5584\tP@10: 0.0871\tR@10: 0.0436\tMAP@10: 0.0361\tNDCG@10: 0.0943\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 54/500:\tLoss: 0.5562\tP@10: 0.0840\tR@10: 0.0420\tMAP@10: 0.0330\tNDCG@10: 0.0883\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 55/500:\tLoss: 0.5573\tP@10: 0.0891\tR@10: 0.0445\tMAP@10: 0.0344\tNDCG@10: 0.0928\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 56/500:\tLoss: 0.5601\tP@10: 0.0785\tR@10: 0.0393\tMAP@10: 0.0286\tNDCG@10: 0.0801\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 57/500:\tLoss: 0.5536\tP@10: 0.0787\tR@10: 0.0393\tMAP@10: 0.0301\tNDCG@10: 0.0824\tHR@10: 0.5458\n",
      "INFO:\ttrainer:\tEpoch 58/500:\tLoss: 0.5406\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0359\tNDCG@10: 0.0937\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 59/500:\tLoss: 0.5445\tP@10: 0.0833\tR@10: 0.0416\tMAP@10: 0.0312\tNDCG@10: 0.0858\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 60/500:\tLoss: 0.5497\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0334\tNDCG@10: 0.0908\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 61/500:\tLoss: 0.5470\tP@10: 0.0812\tR@10: 0.0406\tMAP@10: 0.0308\tNDCG@10: 0.0854\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 62/500:\tLoss: 0.5468\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0331\tNDCG@10: 0.0898\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 63/500:\tLoss: 0.5459\tP@10: 0.0838\tR@10: 0.0419\tMAP@10: 0.0334\tNDCG@10: 0.0893\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 64/500:\tLoss: 0.5455\tP@10: 0.0891\tR@10: 0.0445\tMAP@10: 0.0296\tNDCG@10: 0.0845\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 65/500:\tLoss: 0.5416\tP@10: 0.0912\tR@10: 0.0456\tMAP@10: 0.0319\tNDCG@10: 0.0883\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 66/500:\tLoss: 0.5403\tP@10: 0.0884\tR@10: 0.0442\tMAP@10: 0.0289\tNDCG@10: 0.0828\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 67/500:\tLoss: 0.5398\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0293\tNDCG@10: 0.0833\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 68/500:\tLoss: 0.5430\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0358\tNDCG@10: 0.0943\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 69/500:\tLoss: 0.5389\tP@10: 0.0891\tR@10: 0.0445\tMAP@10: 0.0343\tNDCG@10: 0.0919\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 70/500:\tLoss: 0.5371\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0321\tNDCG@10: 0.0870\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 71/500:\tLoss: 0.5392\tP@10: 0.0815\tR@10: 0.0408\tMAP@10: 0.0311\tNDCG@10: 0.0852\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 72/500:\tLoss: 0.5386\tP@10: 0.0806\tR@10: 0.0403\tMAP@10: 0.0308\tNDCG@10: 0.0848\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 73/500:\tLoss: 0.5359\tP@10: 0.0820\tR@10: 0.0410\tMAP@10: 0.0291\tNDCG@10: 0.0824\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 74/500:\tLoss: 0.5322\tP@10: 0.0824\tR@10: 0.0412\tMAP@10: 0.0310\tNDCG@10: 0.0846\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 75/500:\tLoss: 0.5370\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0313\tNDCG@10: 0.0875\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 76/500:\tLoss: 0.5356\tP@10: 0.0891\tR@10: 0.0445\tMAP@10: 0.0334\tNDCG@10: 0.0913\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 77/500:\tLoss: 0.5301\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0320\tNDCG@10: 0.0891\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 78/500:\tLoss: 0.5373\tP@10: 0.0771\tR@10: 0.0386\tMAP@10: 0.0299\tNDCG@10: 0.0809\tHR@10: 0.5387\n",
      "INFO:\ttrainer:\tEpoch 79/500:\tLoss: 0.5306\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0340\tNDCG@10: 0.0903\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 80/500:\tLoss: 0.5284\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0330\tNDCG@10: 0.0871\tHR@10: 0.5511\n",
      "INFO:\ttrainer:\tEpoch 81/500:\tLoss: 0.5308\tP@10: 0.0787\tR@10: 0.0393\tMAP@10: 0.0318\tNDCG@10: 0.0850\tHR@10: 0.5475\n",
      "INFO:\ttrainer:\tEpoch 82/500:\tLoss: 0.5234\tP@10: 0.0813\tR@10: 0.0407\tMAP@10: 0.0330\tNDCG@10: 0.0872\tHR@10: 0.5546\n",
      "INFO:\ttrainer:\tEpoch 83/500:\tLoss: 0.5228\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0342\tNDCG@10: 0.0909\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 84/500:\tLoss: 0.5277\tP@10: 0.0921\tR@10: 0.0460\tMAP@10: 0.0370\tNDCG@10: 0.0968\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 85/500:\tLoss: 0.5295\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0344\tNDCG@10: 0.0913\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 86/500:\tLoss: 0.5287\tP@10: 0.0884\tR@10: 0.0442\tMAP@10: 0.0352\tNDCG@10: 0.0923\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 87/500:\tLoss: 0.5278\tP@10: 0.0842\tR@10: 0.0421\tMAP@10: 0.0335\tNDCG@10: 0.0891\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 88/500:\tLoss: 0.5302\tP@10: 0.0945\tR@10: 0.0473\tMAP@10: 0.0367\tNDCG@10: 0.0976\tHR@10: 0.6039\n",
      "INFO:\ttrainer:\tEpoch 89/500:\tLoss: 0.5315\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0358\tNDCG@10: 0.0944\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 90/500:\tLoss: 0.5328\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0322\tNDCG@10: 0.0875\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 91/500:\tLoss: 0.5247\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0319\tNDCG@10: 0.0867\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 92/500:\tLoss: 0.5273\tP@10: 0.0868\tR@10: 0.0434\tMAP@10: 0.0334\tNDCG@10: 0.0896\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 93/500:\tLoss: 0.5257\tP@10: 0.0813\tR@10: 0.0407\tMAP@10: 0.0343\tNDCG@10: 0.0899\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 93\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run6/model.pth\n",
      "INFO:\tmain:\tCompleted run 6\n",
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 3,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run7/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run7\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 3 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 2.6859\tP@10: 0.0572\tR@10: 0.0286\tMAP@10: 0.0180\tNDCG@10: 0.0558\tHR@10: 0.4560\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 3.0680\tP@10: 0.0472\tR@10: 0.0236\tMAP@10: 0.0172\tNDCG@10: 0.0503\tHR@10: 0.3803\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 3.5768\tP@10: 0.0435\tR@10: 0.0217\tMAP@10: 0.0137\tNDCG@10: 0.0432\tHR@10: 0.3609\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 3.3051\tP@10: 0.0523\tR@10: 0.0261\tMAP@10: 0.0189\tNDCG@10: 0.0553\tHR@10: 0.4313\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 3.3198\tP@10: 0.0452\tR@10: 0.0226\tMAP@10: 0.0137\tNDCG@10: 0.0443\tHR@10: 0.3926\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 3.3489\tP@10: 0.0551\tR@10: 0.0276\tMAP@10: 0.0136\tNDCG@10: 0.0477\tHR@10: 0.4331\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 3.3092\tP@10: 0.0310\tR@10: 0.0155\tMAP@10: 0.0102\tNDCG@10: 0.0320\tHR@10: 0.2729\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 3.1079\tP@10: 0.0347\tR@10: 0.0173\tMAP@10: 0.0093\tNDCG@10: 0.0312\tHR@10: 0.2782\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 3.1265\tP@10: 0.0488\tR@10: 0.0244\tMAP@10: 0.0187\tNDCG@10: 0.0530\tHR@10: 0.3891\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 3.0584\tP@10: 0.0535\tR@10: 0.0268\tMAP@10: 0.0180\tNDCG@10: 0.0547\tHR@10: 0.4454\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 3.0416\tP@10: 0.0606\tR@10: 0.0303\tMAP@10: 0.0233\tNDCG@10: 0.0662\tHR@10: 0.4806\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 11\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run7/model.pth\n",
      "INFO:\tmain:\tCompleted run 7\n",
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 4,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run8/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run8\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 4 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 2.4375\tP@10: 0.0827\tR@10: 0.0414\tMAP@10: 0.0293\tNDCG@10: 0.0834\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 3.3083\tP@10: 0.0548\tR@10: 0.0274\tMAP@10: 0.0166\tNDCG@10: 0.0521\tHR@10: 0.4349\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 3.4569\tP@10: 0.0563\tR@10: 0.0282\tMAP@10: 0.0175\tNDCG@10: 0.0543\tHR@10: 0.4454\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 3.2916\tP@10: 0.0354\tR@10: 0.0177\tMAP@10: 0.0079\tNDCG@10: 0.0297\tHR@10: 0.3046\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 3.3866\tP@10: 0.0452\tR@10: 0.0226\tMAP@10: 0.0118\tNDCG@10: 0.0405\tHR@10: 0.3768\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 3.2219\tP@10: 0.0690\tR@10: 0.0345\tMAP@10: 0.0253\tNDCG@10: 0.0714\tHR@10: 0.5106\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 3.4179\tP@10: 0.0500\tR@10: 0.0250\tMAP@10: 0.0154\tNDCG@10: 0.0480\tHR@10: 0.4014\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 3.3257\tP@10: 0.0479\tR@10: 0.0239\tMAP@10: 0.0191\tNDCG@10: 0.0543\tHR@10: 0.4032\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 3.1335\tP@10: 0.0717\tR@10: 0.0358\tMAP@10: 0.0262\tNDCG@10: 0.0746\tHR@10: 0.5229\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 3.0374\tP@10: 0.0687\tR@10: 0.0343\tMAP@10: 0.0209\tNDCG@10: 0.0650\tHR@10: 0.5158\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 2.8227\tP@10: 0.0627\tR@10: 0.0313\tMAP@10: 0.0191\tNDCG@10: 0.0586\tHR@10: 0.4542\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 11\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run8/model.pth\n",
      "INFO:\tmain:\tCompleted run 8\n"
     ]
    }
   ],
   "source": [
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-100k/ --model_variant 1\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-100k/ --model_variant 2\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-100k/ --model_variant 3\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-100k/ --model_variant 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed_divide/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 1,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run9/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run9\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 1 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_layer): CrossAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 1.0004\tP@10: 0.0849\tR@10: 0.0424\tMAP@10: 0.0336\tNDCG@10: 0.0889\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 0.9882\tP@10: 0.0192\tR@10: 0.0096\tMAP@10: 0.0070\tNDCG@10: 0.0213\tHR@10: 0.1796\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 1.2135\tP@10: 0.0530\tR@10: 0.0265\tMAP@10: 0.0157\tNDCG@10: 0.0503\tHR@10: 0.4155\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 1.1392\tP@10: 0.0505\tR@10: 0.0253\tMAP@10: 0.0180\tNDCG@10: 0.0531\tHR@10: 0.4049\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 1.0160\tP@10: 0.0577\tR@10: 0.0289\tMAP@10: 0.0210\tNDCG@10: 0.0613\tHR@10: 0.4595\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 0.9226\tP@10: 0.0799\tR@10: 0.0400\tMAP@10: 0.0305\tNDCG@10: 0.0839\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 0.8890\tP@10: 0.0882\tR@10: 0.0441\tMAP@10: 0.0304\tNDCG@10: 0.0876\tHR@10: 0.6232\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 0.8784\tP@10: 0.0790\tR@10: 0.0395\tMAP@10: 0.0258\tNDCG@10: 0.0760\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 0.8288\tP@10: 0.0650\tR@10: 0.0325\tMAP@10: 0.0226\tNDCG@10: 0.0651\tHR@10: 0.4718\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 0.8176\tP@10: 0.0849\tR@10: 0.0424\tMAP@10: 0.0345\tNDCG@10: 0.0914\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 0.8094\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0353\tNDCG@10: 0.0939\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 12/500:\tLoss: 0.7956\tP@10: 0.0838\tR@10: 0.0419\tMAP@10: 0.0319\tNDCG@10: 0.0877\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 13/500:\tLoss: 0.7908\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0329\tNDCG@10: 0.0897\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 14/500:\tLoss: 0.7763\tP@10: 0.0778\tR@10: 0.0389\tMAP@10: 0.0287\tNDCG@10: 0.0798\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 15/500:\tLoss: 0.7606\tP@10: 0.0974\tR@10: 0.0487\tMAP@10: 0.0374\tNDCG@10: 0.0995\tHR@10: 0.6215\n",
      "INFO:\ttrainer:\tEpoch 16/500:\tLoss: 0.7551\tP@10: 0.0926\tR@10: 0.0463\tMAP@10: 0.0349\tNDCG@10: 0.0947\tHR@10: 0.6109\n",
      "INFO:\ttrainer:\tEpoch 17/500:\tLoss: 0.7453\tP@10: 0.0822\tR@10: 0.0411\tMAP@10: 0.0307\tNDCG@10: 0.0842\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 18/500:\tLoss: 0.7136\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0310\tNDCG@10: 0.0850\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 19/500:\tLoss: 0.7060\tP@10: 0.0794\tR@10: 0.0397\tMAP@10: 0.0279\tNDCG@10: 0.0794\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 20/500:\tLoss: 0.6963\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0339\tNDCG@10: 0.0914\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 21/500:\tLoss: 0.6808\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0349\tNDCG@10: 0.0932\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 22/500:\tLoss: 0.6776\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0362\tNDCG@10: 0.0964\tHR@10: 0.6092\n",
      "INFO:\ttrainer:\tEpoch 23/500:\tLoss: 0.6671\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0333\tNDCG@10: 0.0908\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 24/500:\tLoss: 0.6541\tP@10: 0.0838\tR@10: 0.0419\tMAP@10: 0.0290\tNDCG@10: 0.0825\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 25/500:\tLoss: 0.6570\tP@10: 0.0819\tR@10: 0.0409\tMAP@10: 0.0309\tNDCG@10: 0.0850\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 26/500:\tLoss: 0.6485\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0328\tNDCG@10: 0.0893\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 27/500:\tLoss: 0.6372\tP@10: 0.0861\tR@10: 0.0430\tMAP@10: 0.0345\tNDCG@10: 0.0919\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 28/500:\tLoss: 0.6269\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0365\tNDCG@10: 0.0961\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 29/500:\tLoss: 0.6335\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0352\tNDCG@10: 0.0938\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 30/500:\tLoss: 0.6254\tP@10: 0.0815\tR@10: 0.0408\tMAP@10: 0.0307\tNDCG@10: 0.0856\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 31/500:\tLoss: 0.6216\tP@10: 0.0819\tR@10: 0.0409\tMAP@10: 0.0314\tNDCG@10: 0.0863\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 32/500:\tLoss: 0.6138\tP@10: 0.0843\tR@10: 0.0422\tMAP@10: 0.0330\tNDCG@10: 0.0892\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 33/500:\tLoss: 0.6158\tP@10: 0.0919\tR@10: 0.0460\tMAP@10: 0.0345\tNDCG@10: 0.0936\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 34/500:\tLoss: 0.6146\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0342\tNDCG@10: 0.0921\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 35/500:\tLoss: 0.6061\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0341\tNDCG@10: 0.0923\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 36/500:\tLoss: 0.6047\tP@10: 0.0842\tR@10: 0.0421\tMAP@10: 0.0321\tNDCG@10: 0.0879\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 37/500:\tLoss: 0.5967\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0347\tNDCG@10: 0.0929\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 38/500:\tLoss: 0.5886\tP@10: 0.0917\tR@10: 0.0459\tMAP@10: 0.0347\tNDCG@10: 0.0938\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 39/500:\tLoss: 0.5858\tP@10: 0.0854\tR@10: 0.0427\tMAP@10: 0.0306\tNDCG@10: 0.0858\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 40/500:\tLoss: 0.5883\tP@10: 0.0850\tR@10: 0.0425\tMAP@10: 0.0337\tNDCG@10: 0.0910\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 41/500:\tLoss: 0.5837\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0358\tNDCG@10: 0.0948\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 42/500:\tLoss: 0.5818\tP@10: 0.0812\tR@10: 0.0406\tMAP@10: 0.0318\tNDCG@10: 0.0856\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 43/500:\tLoss: 0.5800\tP@10: 0.0820\tR@10: 0.0410\tMAP@10: 0.0335\tNDCG@10: 0.0881\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 44/500:\tLoss: 0.5715\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0353\tNDCG@10: 0.0943\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 45/500:\tLoss: 0.5734\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0348\tNDCG@10: 0.0937\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 46/500:\tLoss: 0.5820\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0355\tNDCG@10: 0.0945\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 47/500:\tLoss: 0.5707\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0347\tNDCG@10: 0.0928\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 48/500:\tLoss: 0.5674\tP@10: 0.0847\tR@10: 0.0423\tMAP@10: 0.0292\tNDCG@10: 0.0831\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 49/500:\tLoss: 0.5700\tP@10: 0.0819\tR@10: 0.0409\tMAP@10: 0.0319\tNDCG@10: 0.0856\tHR@10: 0.5387\n",
      "INFO:\ttrainer:\tEpoch 50/500:\tLoss: 0.5726\tP@10: 0.0898\tR@10: 0.0449\tMAP@10: 0.0334\tNDCG@10: 0.0906\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 51/500:\tLoss: 0.5649\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0361\tNDCG@10: 0.0957\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 52/500:\tLoss: 0.5651\tP@10: 0.0935\tR@10: 0.0467\tMAP@10: 0.0363\tNDCG@10: 0.0967\tHR@10: 0.6056\n",
      "INFO:\ttrainer:\tEpoch 53/500:\tLoss: 0.5598\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0358\tNDCG@10: 0.0959\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 54/500:\tLoss: 0.5574\tP@10: 0.0910\tR@10: 0.0455\tMAP@10: 0.0362\tNDCG@10: 0.0960\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 55/500:\tLoss: 0.5634\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0337\tNDCG@10: 0.0908\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 56/500:\tLoss: 0.5549\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0346\tNDCG@10: 0.0918\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 57/500:\tLoss: 0.5531\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0340\tNDCG@10: 0.0913\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 58/500:\tLoss: 0.5557\tP@10: 0.0933\tR@10: 0.0467\tMAP@10: 0.0374\tNDCG@10: 0.0987\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 59/500:\tLoss: 0.5578\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0353\tNDCG@10: 0.0949\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 60/500:\tLoss: 0.5539\tP@10: 0.0933\tR@10: 0.0467\tMAP@10: 0.0379\tNDCG@10: 0.0991\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 61/500:\tLoss: 0.5526\tP@10: 0.0954\tR@10: 0.0477\tMAP@10: 0.0372\tNDCG@10: 0.0981\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 62/500:\tLoss: 0.5487\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0331\tNDCG@10: 0.0902\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 63/500:\tLoss: 0.5488\tP@10: 0.0847\tR@10: 0.0423\tMAP@10: 0.0344\tNDCG@10: 0.0912\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 64/500:\tLoss: 0.5471\tP@10: 0.0935\tR@10: 0.0467\tMAP@10: 0.0360\tNDCG@10: 0.0976\tHR@10: 0.6162\n",
      "INFO:\ttrainer:\tEpoch 65/500:\tLoss: 0.5397\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0341\tNDCG@10: 0.0923\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 66/500:\tLoss: 0.5423\tP@10: 0.0924\tR@10: 0.0462\tMAP@10: 0.0372\tNDCG@10: 0.0987\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 67/500:\tLoss: 0.5437\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0349\tNDCG@10: 0.0930\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 68/500:\tLoss: 0.5415\tP@10: 0.0912\tR@10: 0.0456\tMAP@10: 0.0365\tNDCG@10: 0.0973\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 69/500:\tLoss: 0.5403\tP@10: 0.0930\tR@10: 0.0465\tMAP@10: 0.0379\tNDCG@10: 0.0999\tHR@10: 0.6180\n",
      "INFO:\ttrainer:\tEpoch 70/500:\tLoss: 0.5440\tP@10: 0.0958\tR@10: 0.0479\tMAP@10: 0.0375\tNDCG@10: 0.1002\tHR@10: 0.6250\n",
      "INFO:\ttrainer:\tEpoch 71/500:\tLoss: 0.5417\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0349\tNDCG@10: 0.0927\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 72/500:\tLoss: 0.5391\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0343\tNDCG@10: 0.0915\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 73/500:\tLoss: 0.5418\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0319\tNDCG@10: 0.0876\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 74/500:\tLoss: 0.5417\tP@10: 0.0910\tR@10: 0.0455\tMAP@10: 0.0324\tNDCG@10: 0.0905\tHR@10: 0.6144\n",
      "INFO:\ttrainer:\tEpoch 75/500:\tLoss: 0.5396\tP@10: 0.0910\tR@10: 0.0455\tMAP@10: 0.0299\tNDCG@10: 0.0863\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 76/500:\tLoss: 0.5337\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0301\tNDCG@10: 0.0866\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 77/500:\tLoss: 0.5321\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0297\tNDCG@10: 0.0853\tHR@10: 0.6039\n",
      "INFO:\ttrainer:\tEpoch 78/500:\tLoss: 0.5333\tP@10: 0.0796\tR@10: 0.0398\tMAP@10: 0.0267\tNDCG@10: 0.0775\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 79/500:\tLoss: 0.5409\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0318\tNDCG@10: 0.0879\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 80/500:\tLoss: 0.5307\tP@10: 0.0919\tR@10: 0.0460\tMAP@10: 0.0355\tNDCG@10: 0.0954\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 81/500:\tLoss: 0.5331\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0360\tNDCG@10: 0.0959\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 82/500:\tLoss: 0.5302\tP@10: 0.0840\tR@10: 0.0420\tMAP@10: 0.0280\tNDCG@10: 0.0802\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 83/500:\tLoss: 0.5273\tP@10: 0.0871\tR@10: 0.0436\tMAP@10: 0.0285\tNDCG@10: 0.0825\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 84/500:\tLoss: 0.5285\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0314\tNDCG@10: 0.0863\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 85/500:\tLoss: 0.5259\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0346\tNDCG@10: 0.0918\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 86/500:\tLoss: 0.5266\tP@10: 0.0940\tR@10: 0.0470\tMAP@10: 0.0351\tNDCG@10: 0.0946\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 87/500:\tLoss: 0.5302\tP@10: 0.0924\tR@10: 0.0462\tMAP@10: 0.0344\tNDCG@10: 0.0937\tHR@10: 0.6074\n",
      "INFO:\ttrainer:\tEpoch 88/500:\tLoss: 0.5262\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0351\tNDCG@10: 0.0945\tHR@10: 0.6056\n",
      "INFO:\ttrainer:\tEpoch 89/500:\tLoss: 0.5245\tP@10: 0.0951\tR@10: 0.0475\tMAP@10: 0.0369\tNDCG@10: 0.0987\tHR@10: 0.6039\n",
      "INFO:\ttrainer:\tEpoch 90/500:\tLoss: 0.5295\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0357\tNDCG@10: 0.0946\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 91/500:\tLoss: 0.5248\tP@10: 0.0933\tR@10: 0.0467\tMAP@10: 0.0365\tNDCG@10: 0.0964\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 92/500:\tLoss: 0.5178\tP@10: 0.0937\tR@10: 0.0468\tMAP@10: 0.0365\tNDCG@10: 0.0972\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 93/500:\tLoss: 0.5186\tP@10: 0.0919\tR@10: 0.0460\tMAP@10: 0.0309\tNDCG@10: 0.0880\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 94/500:\tLoss: 0.5229\tP@10: 0.0891\tR@10: 0.0445\tMAP@10: 0.0304\tNDCG@10: 0.0862\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 95/500:\tLoss: 0.5298\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0309\tNDCG@10: 0.0871\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 96/500:\tLoss: 0.5238\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0315\tNDCG@10: 0.0888\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 97/500:\tLoss: 0.5240\tP@10: 0.0907\tR@10: 0.0453\tMAP@10: 0.0360\tNDCG@10: 0.0954\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 98/500:\tLoss: 0.5225\tP@10: 0.0917\tR@10: 0.0459\tMAP@10: 0.0354\tNDCG@10: 0.0948\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 99/500:\tLoss: 0.5201\tP@10: 0.0845\tR@10: 0.0423\tMAP@10: 0.0333\tNDCG@10: 0.0887\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 100/500:\tLoss: 0.5204\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0319\tNDCG@10: 0.0860\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 101/500:\tLoss: 0.5177\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0334\tNDCG@10: 0.0920\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 102/500:\tLoss: 0.5161\tP@10: 0.0880\tR@10: 0.0440\tMAP@10: 0.0350\tNDCG@10: 0.0929\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 103/500:\tLoss: 0.5129\tP@10: 0.0871\tR@10: 0.0436\tMAP@10: 0.0324\tNDCG@10: 0.0876\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 104/500:\tLoss: 0.5197\tP@10: 0.0798\tR@10: 0.0399\tMAP@10: 0.0303\tNDCG@10: 0.0827\tHR@10: 0.5511\n",
      "INFO:\ttrainer:\tEpoch 105/500:\tLoss: 0.5199\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0333\tNDCG@10: 0.0913\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 106/500:\tLoss: 0.5183\tP@10: 0.0840\tR@10: 0.0420\tMAP@10: 0.0319\tNDCG@10: 0.0872\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 107/500:\tLoss: 0.5174\tP@10: 0.0877\tR@10: 0.0438\tMAP@10: 0.0318\tNDCG@10: 0.0885\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 108/500:\tLoss: 0.5127\tP@10: 0.0917\tR@10: 0.0459\tMAP@10: 0.0315\tNDCG@10: 0.0889\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 109/500:\tLoss: 0.5164\tP@10: 0.0917\tR@10: 0.0459\tMAP@10: 0.0334\tNDCG@10: 0.0913\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 110/500:\tLoss: 0.5159\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0355\tNDCG@10: 0.0944\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 111/500:\tLoss: 0.5126\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0345\tNDCG@10: 0.0921\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 112/500:\tLoss: 0.5159\tP@10: 0.0940\tR@10: 0.0470\tMAP@10: 0.0377\tNDCG@10: 0.0995\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 113/500:\tLoss: 0.5155\tP@10: 0.0898\tR@10: 0.0449\tMAP@10: 0.0361\tNDCG@10: 0.0956\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 114/500:\tLoss: 0.5135\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0359\tNDCG@10: 0.0941\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 115/500:\tLoss: 0.5155\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0358\tNDCG@10: 0.0949\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 116/500:\tLoss: 0.5128\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0350\tNDCG@10: 0.0940\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 117/500:\tLoss: 0.5121\tP@10: 0.0842\tR@10: 0.0421\tMAP@10: 0.0303\tNDCG@10: 0.0851\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 118/500:\tLoss: 0.5143\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0312\tNDCG@10: 0.0871\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 119/500:\tLoss: 0.5087\tP@10: 0.0812\tR@10: 0.0406\tMAP@10: 0.0288\tNDCG@10: 0.0806\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 120/500:\tLoss: 0.5109\tP@10: 0.0806\tR@10: 0.0403\tMAP@10: 0.0292\tNDCG@10: 0.0817\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 121/500:\tLoss: 0.5131\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0283\tNDCG@10: 0.0817\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 122/500:\tLoss: 0.5070\tP@10: 0.0882\tR@10: 0.0441\tMAP@10: 0.0280\tNDCG@10: 0.0821\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 123/500:\tLoss: 0.5112\tP@10: 0.0871\tR@10: 0.0436\tMAP@10: 0.0280\tNDCG@10: 0.0819\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 124/500:\tLoss: 0.5145\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0312\tNDCG@10: 0.0855\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 125/500:\tLoss: 0.5141\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0305\tNDCG@10: 0.0839\tHR@10: 0.5546\n",
      "INFO:\ttrainer:\tEpoch 126/500:\tLoss: 0.5100\tP@10: 0.0840\tR@10: 0.0420\tMAP@10: 0.0333\tNDCG@10: 0.0887\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 127/500:\tLoss: 0.5074\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0346\tNDCG@10: 0.0913\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 128/500:\tLoss: 0.5115\tP@10: 0.0843\tR@10: 0.0422\tMAP@10: 0.0331\tNDCG@10: 0.0876\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 129/500:\tLoss: 0.5069\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0335\tNDCG@10: 0.0907\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 130/500:\tLoss: 0.5079\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0343\tNDCG@10: 0.0921\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 131/500:\tLoss: 0.5086\tP@10: 0.0887\tR@10: 0.0444\tMAP@10: 0.0348\tNDCG@10: 0.0929\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 132/500:\tLoss: 0.5070\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0322\tNDCG@10: 0.0867\tHR@10: 0.5546\n",
      "INFO:\ttrainer:\tEpoch 133/500:\tLoss: 0.5083\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0345\tNDCG@10: 0.0931\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 134/500:\tLoss: 0.5037\tP@10: 0.0863\tR@10: 0.0431\tMAP@10: 0.0321\tNDCG@10: 0.0881\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 135/500:\tLoss: 0.5094\tP@10: 0.0884\tR@10: 0.0442\tMAP@10: 0.0350\tNDCG@10: 0.0927\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 136/500:\tLoss: 0.5095\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0333\tNDCG@10: 0.0900\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 137/500:\tLoss: 0.5050\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0329\tNDCG@10: 0.0887\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 138/500:\tLoss: 0.5073\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0324\tNDCG@10: 0.0892\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 139/500:\tLoss: 0.5090\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0332\tNDCG@10: 0.0896\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 140/500:\tLoss: 0.5091\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0321\tNDCG@10: 0.0865\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 141/500:\tLoss: 0.5030\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0329\tNDCG@10: 0.0908\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 142/500:\tLoss: 0.5053\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0313\tNDCG@10: 0.0875\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 143/500:\tLoss: 0.5093\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0344\tNDCG@10: 0.0928\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 144/500:\tLoss: 0.5064\tP@10: 0.0822\tR@10: 0.0411\tMAP@10: 0.0312\tNDCG@10: 0.0850\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 145/500:\tLoss: 0.5112\tP@10: 0.0803\tR@10: 0.0401\tMAP@10: 0.0316\tNDCG@10: 0.0853\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 146/500:\tLoss: 0.5031\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0332\tNDCG@10: 0.0895\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 147/500:\tLoss: 0.5056\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0301\tNDCG@10: 0.0852\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 148/500:\tLoss: 0.5064\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0344\tNDCG@10: 0.0928\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 149/500:\tLoss: 0.4997\tP@10: 0.0877\tR@10: 0.0438\tMAP@10: 0.0351\tNDCG@10: 0.0932\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 150/500:\tLoss: 0.5007\tP@10: 0.0880\tR@10: 0.0440\tMAP@10: 0.0324\tNDCG@10: 0.0890\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 151/500:\tLoss: 0.5056\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0338\tNDCG@10: 0.0918\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 152/500:\tLoss: 0.5068\tP@10: 0.0861\tR@10: 0.0430\tMAP@10: 0.0328\tNDCG@10: 0.0886\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 153/500:\tLoss: 0.5010\tP@10: 0.0907\tR@10: 0.0453\tMAP@10: 0.0347\tNDCG@10: 0.0925\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 154/500:\tLoss: 0.5027\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0344\tNDCG@10: 0.0916\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 155/500:\tLoss: 0.5108\tP@10: 0.0921\tR@10: 0.0460\tMAP@10: 0.0327\tNDCG@10: 0.0906\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 156/500:\tLoss: 0.5006\tP@10: 0.0815\tR@10: 0.0408\tMAP@10: 0.0329\tNDCG@10: 0.0867\tHR@10: 0.5440\n",
      "INFO:\ttrainer:\tEpoch 157/500:\tLoss: 0.5038\tP@10: 0.0835\tR@10: 0.0417\tMAP@10: 0.0331\tNDCG@10: 0.0877\tHR@10: 0.5493\n",
      "INFO:\ttrainer:\tEpoch 158/500:\tLoss: 0.5042\tP@10: 0.0824\tR@10: 0.0412\tMAP@10: 0.0328\tNDCG@10: 0.0877\tHR@10: 0.5546\n",
      "INFO:\ttrainer:\tEpoch 159/500:\tLoss: 0.5050\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0334\tNDCG@10: 0.0896\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 159\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run9/model.pth\n",
      "INFO:\tmain:\tCompleted run 9\n",
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed_divide/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 2,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run10/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run10\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 2 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_layer): CrossAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 1.0170\tP@10: 0.0442\tR@10: 0.0221\tMAP@10: 0.0139\tNDCG@10: 0.0439\tHR@10: 0.3785\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 1.0608\tP@10: 0.0493\tR@10: 0.0246\tMAP@10: 0.0124\tNDCG@10: 0.0431\tHR@10: 0.3979\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 1.0455\tP@10: 0.0521\tR@10: 0.0261\tMAP@10: 0.0183\tNDCG@10: 0.0541\tHR@10: 0.4102\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 1.0762\tP@10: 0.0563\tR@10: 0.0282\tMAP@10: 0.0229\tNDCG@10: 0.0628\tHR@10: 0.4296\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 0.9779\tP@10: 0.0710\tR@10: 0.0355\tMAP@10: 0.0234\tNDCG@10: 0.0681\tHR@10: 0.5123\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 0.8998\tP@10: 0.0579\tR@10: 0.0290\tMAP@10: 0.0191\tNDCG@10: 0.0567\tHR@10: 0.4384\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 0.8739\tP@10: 0.0688\tR@10: 0.0344\tMAP@10: 0.0262\tNDCG@10: 0.0729\tHR@10: 0.5141\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 0.8422\tP@10: 0.0782\tR@10: 0.0391\tMAP@10: 0.0272\tNDCG@10: 0.0766\tHR@10: 0.5282\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 0.8316\tP@10: 0.0717\tR@10: 0.0358\tMAP@10: 0.0309\tNDCG@10: 0.0814\tHR@10: 0.5194\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 0.8026\tP@10: 0.0768\tR@10: 0.0384\tMAP@10: 0.0314\tNDCG@10: 0.0847\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 0.8015\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0327\tNDCG@10: 0.0890\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 12/500:\tLoss: 0.7756\tP@10: 0.0845\tR@10: 0.0423\tMAP@10: 0.0346\tNDCG@10: 0.0917\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 13/500:\tLoss: 0.7676\tP@10: 0.0845\tR@10: 0.0423\tMAP@10: 0.0345\tNDCG@10: 0.0915\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 14/500:\tLoss: 0.7501\tP@10: 0.0789\tR@10: 0.0394\tMAP@10: 0.0308\tNDCG@10: 0.0841\tHR@10: 0.5475\n",
      "INFO:\ttrainer:\tEpoch 15/500:\tLoss: 0.7422\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0273\tNDCG@10: 0.0815\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 16/500:\tLoss: 0.7233\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0329\tNDCG@10: 0.0894\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 17/500:\tLoss: 0.7164\tP@10: 0.1011\tR@10: 0.0505\tMAP@10: 0.0379\tNDCG@10: 0.1023\tHR@10: 0.6444\n",
      "INFO:\ttrainer:\tEpoch 18/500:\tLoss: 0.7068\tP@10: 0.0935\tR@10: 0.0467\tMAP@10: 0.0358\tNDCG@10: 0.0975\tHR@10: 0.6162\n",
      "INFO:\ttrainer:\tEpoch 19/500:\tLoss: 0.6942\tP@10: 0.0871\tR@10: 0.0436\tMAP@10: 0.0343\tNDCG@10: 0.0923\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 20/500:\tLoss: 0.6849\tP@10: 0.0845\tR@10: 0.0423\tMAP@10: 0.0346\tNDCG@10: 0.0908\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 21/500:\tLoss: 0.6785\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0371\tNDCG@10: 0.0981\tHR@10: 0.6232\n",
      "INFO:\ttrainer:\tEpoch 22/500:\tLoss: 0.6713\tP@10: 0.0933\tR@10: 0.0467\tMAP@10: 0.0349\tNDCG@10: 0.0957\tHR@10: 0.6197\n",
      "INFO:\ttrainer:\tEpoch 23/500:\tLoss: 0.6604\tP@10: 0.0805\tR@10: 0.0402\tMAP@10: 0.0297\tNDCG@10: 0.0826\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 24/500:\tLoss: 0.6558\tP@10: 0.0859\tR@10: 0.0430\tMAP@10: 0.0266\tNDCG@10: 0.0791\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 25/500:\tLoss: 0.6449\tP@10: 0.0840\tR@10: 0.0420\tMAP@10: 0.0324\tNDCG@10: 0.0876\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 26/500:\tLoss: 0.6341\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0306\tNDCG@10: 0.0845\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 27/500:\tLoss: 0.6327\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0341\tNDCG@10: 0.0914\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 28/500:\tLoss: 0.6229\tP@10: 0.0880\tR@10: 0.0440\tMAP@10: 0.0327\tNDCG@10: 0.0896\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 29/500:\tLoss: 0.6209\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0338\tNDCG@10: 0.0917\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 30/500:\tLoss: 0.6179\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0358\tNDCG@10: 0.0952\tHR@10: 0.6021\n",
      "INFO:\ttrainer:\tEpoch 31/500:\tLoss: 0.6142\tP@10: 0.0748\tR@10: 0.0374\tMAP@10: 0.0315\tNDCG@10: 0.0835\tHR@10: 0.5387\n",
      "INFO:\ttrainer:\tEpoch 32/500:\tLoss: 0.6152\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0344\tNDCG@10: 0.0920\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 33/500:\tLoss: 0.6082\tP@10: 0.0824\tR@10: 0.0412\tMAP@10: 0.0323\tNDCG@10: 0.0870\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 34/500:\tLoss: 0.6054\tP@10: 0.0731\tR@10: 0.0365\tMAP@10: 0.0293\tNDCG@10: 0.0795\tHR@10: 0.5246\n",
      "INFO:\ttrainer:\tEpoch 35/500:\tLoss: 0.5981\tP@10: 0.0775\tR@10: 0.0387\tMAP@10: 0.0315\tNDCG@10: 0.0843\tHR@10: 0.5387\n",
      "INFO:\ttrainer:\tEpoch 36/500:\tLoss: 0.5916\tP@10: 0.0801\tR@10: 0.0401\tMAP@10: 0.0321\tNDCG@10: 0.0860\tHR@10: 0.5546\n",
      "INFO:\ttrainer:\tEpoch 37/500:\tLoss: 0.5900\tP@10: 0.0882\tR@10: 0.0441\tMAP@10: 0.0343\tNDCG@10: 0.0920\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 38/500:\tLoss: 0.5854\tP@10: 0.0919\tR@10: 0.0460\tMAP@10: 0.0325\tNDCG@10: 0.0901\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 39/500:\tLoss: 0.5898\tP@10: 0.0761\tR@10: 0.0380\tMAP@10: 0.0309\tNDCG@10: 0.0819\tHR@10: 0.5264\n",
      "INFO:\ttrainer:\tEpoch 40/500:\tLoss: 0.5820\tP@10: 0.0863\tR@10: 0.0431\tMAP@10: 0.0345\tNDCG@10: 0.0906\tHR@10: 0.5511\n",
      "INFO:\ttrainer:\tEpoch 41/500:\tLoss: 0.5874\tP@10: 0.0880\tR@10: 0.0440\tMAP@10: 0.0354\tNDCG@10: 0.0941\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 42/500:\tLoss: 0.5839\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0329\tNDCG@10: 0.0903\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 43/500:\tLoss: 0.5771\tP@10: 0.0898\tR@10: 0.0449\tMAP@10: 0.0328\tNDCG@10: 0.0900\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 44/500:\tLoss: 0.5706\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0342\tNDCG@10: 0.0927\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 45/500:\tLoss: 0.5671\tP@10: 0.0864\tR@10: 0.0432\tMAP@10: 0.0316\tNDCG@10: 0.0871\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 46/500:\tLoss: 0.5660\tP@10: 0.0931\tR@10: 0.0466\tMAP@10: 0.0361\tNDCG@10: 0.0963\tHR@10: 0.6056\n",
      "INFO:\ttrainer:\tEpoch 47/500:\tLoss: 0.5673\tP@10: 0.0958\tR@10: 0.0479\tMAP@10: 0.0376\tNDCG@10: 0.0998\tHR@10: 0.6144\n",
      "INFO:\ttrainer:\tEpoch 48/500:\tLoss: 0.5622\tP@10: 0.0930\tR@10: 0.0465\tMAP@10: 0.0357\tNDCG@10: 0.0957\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 49/500:\tLoss: 0.5663\tP@10: 0.0884\tR@10: 0.0442\tMAP@10: 0.0351\tNDCG@10: 0.0930\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 50/500:\tLoss: 0.5593\tP@10: 0.0882\tR@10: 0.0441\tMAP@10: 0.0335\tNDCG@10: 0.0907\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 51/500:\tLoss: 0.5574\tP@10: 0.0868\tR@10: 0.0434\tMAP@10: 0.0336\tNDCG@10: 0.0909\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 52/500:\tLoss: 0.5567\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0344\tNDCG@10: 0.0916\tHR@10: 0.5634\n",
      "INFO:\ttrainer:\tEpoch 53/500:\tLoss: 0.5554\tP@10: 0.0940\tR@10: 0.0470\tMAP@10: 0.0357\tNDCG@10: 0.0959\tHR@10: 0.6092\n",
      "INFO:\ttrainer:\tEpoch 54/500:\tLoss: 0.5542\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0345\tNDCG@10: 0.0912\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 55/500:\tLoss: 0.5535\tP@10: 0.0877\tR@10: 0.0438\tMAP@10: 0.0320\tNDCG@10: 0.0883\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 56/500:\tLoss: 0.5529\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0331\tNDCG@10: 0.0909\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 57/500:\tLoss: 0.5580\tP@10: 0.0880\tR@10: 0.0440\tMAP@10: 0.0356\tNDCG@10: 0.0936\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 58/500:\tLoss: 0.5487\tP@10: 0.0908\tR@10: 0.0454\tMAP@10: 0.0354\tNDCG@10: 0.0933\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 59/500:\tLoss: 0.5542\tP@10: 0.0827\tR@10: 0.0414\tMAP@10: 0.0292\tNDCG@10: 0.0812\tHR@10: 0.5493\n",
      "INFO:\ttrainer:\tEpoch 60/500:\tLoss: 0.5448\tP@10: 0.0923\tR@10: 0.0461\tMAP@10: 0.0328\tNDCG@10: 0.0906\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 61/500:\tLoss: 0.5464\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0348\tNDCG@10: 0.0941\tHR@10: 0.5986\n",
      "INFO:\ttrainer:\tEpoch 62/500:\tLoss: 0.5564\tP@10: 0.0923\tR@10: 0.0461\tMAP@10: 0.0367\tNDCG@10: 0.0963\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 63/500:\tLoss: 0.5491\tP@10: 0.0912\tR@10: 0.0456\tMAP@10: 0.0358\tNDCG@10: 0.0951\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 64/500:\tLoss: 0.5475\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0356\tNDCG@10: 0.0942\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 65/500:\tLoss: 0.5438\tP@10: 0.0905\tR@10: 0.0452\tMAP@10: 0.0361\tNDCG@10: 0.0955\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 66/500:\tLoss: 0.5398\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0367\tNDCG@10: 0.0969\tHR@10: 0.5863\n",
      "INFO:\ttrainer:\tEpoch 67/500:\tLoss: 0.5435\tP@10: 0.0787\tR@10: 0.0393\tMAP@10: 0.0296\tNDCG@10: 0.0816\tHR@10: 0.5475\n",
      "INFO:\ttrainer:\tEpoch 68/500:\tLoss: 0.5391\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0338\tNDCG@10: 0.0893\tHR@10: 0.5511\n",
      "INFO:\ttrainer:\tEpoch 69/500:\tLoss: 0.5375\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0327\tNDCG@10: 0.0872\tHR@10: 0.5511\n",
      "INFO:\ttrainer:\tEpoch 70/500:\tLoss: 0.5336\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0343\tNDCG@10: 0.0917\tHR@10: 0.5792\n",
      "INFO:\ttrainer:\tEpoch 71/500:\tLoss: 0.5381\tP@10: 0.0910\tR@10: 0.0455\tMAP@10: 0.0339\tNDCG@10: 0.0923\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 72/500:\tLoss: 0.5322\tP@10: 0.0944\tR@10: 0.0472\tMAP@10: 0.0360\tNDCG@10: 0.0967\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 73/500:\tLoss: 0.5316\tP@10: 0.0926\tR@10: 0.0463\tMAP@10: 0.0357\tNDCG@10: 0.0954\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 74/500:\tLoss: 0.5332\tP@10: 0.0863\tR@10: 0.0431\tMAP@10: 0.0334\tNDCG@10: 0.0899\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 75/500:\tLoss: 0.5315\tP@10: 0.0930\tR@10: 0.0465\tMAP@10: 0.0350\tNDCG@10: 0.0947\tHR@10: 0.6004\n",
      "INFO:\ttrainer:\tEpoch 76/500:\tLoss: 0.5283\tP@10: 0.0903\tR@10: 0.0452\tMAP@10: 0.0351\tNDCG@10: 0.0938\tHR@10: 0.5757\n",
      "INFO:\ttrainer:\tEpoch 77/500:\tLoss: 0.5304\tP@10: 0.0900\tR@10: 0.0450\tMAP@10: 0.0343\tNDCG@10: 0.0926\tHR@10: 0.5933\n",
      "INFO:\ttrainer:\tEpoch 78/500:\tLoss: 0.5310\tP@10: 0.0951\tR@10: 0.0475\tMAP@10: 0.0355\tNDCG@10: 0.0959\tHR@10: 0.6039\n",
      "INFO:\ttrainer:\tEpoch 79/500:\tLoss: 0.5336\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0341\tNDCG@10: 0.0923\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 80/500:\tLoss: 0.5306\tP@10: 0.0831\tR@10: 0.0415\tMAP@10: 0.0336\tNDCG@10: 0.0889\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 81/500:\tLoss: 0.5276\tP@10: 0.0903\tR@10: 0.0452\tMAP@10: 0.0351\tNDCG@10: 0.0933\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 82/500:\tLoss: 0.5268\tP@10: 0.0812\tR@10: 0.0406\tMAP@10: 0.0320\tNDCG@10: 0.0861\tHR@10: 0.5563\n",
      "INFO:\ttrainer:\tEpoch 83/500:\tLoss: 0.5236\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0339\tNDCG@10: 0.0914\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 84/500:\tLoss: 0.5251\tP@10: 0.0856\tR@10: 0.0428\tMAP@10: 0.0337\tNDCG@10: 0.0898\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 85/500:\tLoss: 0.5274\tP@10: 0.0836\tR@10: 0.0418\tMAP@10: 0.0339\tNDCG@10: 0.0888\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 86/500:\tLoss: 0.5260\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0344\tNDCG@10: 0.0911\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 87/500:\tLoss: 0.5251\tP@10: 0.0956\tR@10: 0.0478\tMAP@10: 0.0366\tNDCG@10: 0.0972\tHR@10: 0.6039\n",
      "INFO:\ttrainer:\tEpoch 88/500:\tLoss: 0.5260\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0342\tNDCG@10: 0.0927\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 89/500:\tLoss: 0.5212\tP@10: 0.0915\tR@10: 0.0458\tMAP@10: 0.0343\tNDCG@10: 0.0927\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 90/500:\tLoss: 0.5278\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0359\tNDCG@10: 0.0930\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 91/500:\tLoss: 0.5218\tP@10: 0.0912\tR@10: 0.0456\tMAP@10: 0.0366\tNDCG@10: 0.0963\tHR@10: 0.5968\n",
      "INFO:\ttrainer:\tEpoch 92/500:\tLoss: 0.5222\tP@10: 0.0928\tR@10: 0.0464\tMAP@10: 0.0368\tNDCG@10: 0.0965\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 93/500:\tLoss: 0.5143\tP@10: 0.0923\tR@10: 0.0461\tMAP@10: 0.0359\tNDCG@10: 0.0950\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 94/500:\tLoss: 0.5256\tP@10: 0.0850\tR@10: 0.0425\tMAP@10: 0.0338\tNDCG@10: 0.0900\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 95/500:\tLoss: 0.5234\tP@10: 0.0898\tR@10: 0.0449\tMAP@10: 0.0346\tNDCG@10: 0.0929\tHR@10: 0.5915\n",
      "INFO:\ttrainer:\tEpoch 96/500:\tLoss: 0.5163\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0354\tNDCG@10: 0.0938\tHR@10: 0.5880\n",
      "INFO:\ttrainer:\tEpoch 97/500:\tLoss: 0.5262\tP@10: 0.0903\tR@10: 0.0452\tMAP@10: 0.0349\tNDCG@10: 0.0938\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 98/500:\tLoss: 0.5248\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0345\tNDCG@10: 0.0919\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 99/500:\tLoss: 0.5201\tP@10: 0.0893\tR@10: 0.0446\tMAP@10: 0.0345\tNDCG@10: 0.0916\tHR@10: 0.5827\n",
      "INFO:\ttrainer:\tEpoch 100/500:\tLoss: 0.5198\tP@10: 0.0817\tR@10: 0.0408\tMAP@10: 0.0328\tNDCG@10: 0.0876\tHR@10: 0.5493\n",
      "INFO:\ttrainer:\tEpoch 101/500:\tLoss: 0.5129\tP@10: 0.0912\tR@10: 0.0456\tMAP@10: 0.0354\tNDCG@10: 0.0947\tHR@10: 0.5951\n",
      "INFO:\ttrainer:\tEpoch 102/500:\tLoss: 0.5183\tP@10: 0.0882\tR@10: 0.0441\tMAP@10: 0.0354\tNDCG@10: 0.0937\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 103/500:\tLoss: 0.5146\tP@10: 0.0873\tR@10: 0.0437\tMAP@10: 0.0326\tNDCG@10: 0.0888\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEpoch 104/500:\tLoss: 0.5163\tP@10: 0.0822\tR@10: 0.0411\tMAP@10: 0.0309\tNDCG@10: 0.0850\tHR@10: 0.5616\n",
      "INFO:\ttrainer:\tEpoch 105/500:\tLoss: 0.5165\tP@10: 0.0852\tR@10: 0.0426\tMAP@10: 0.0318\tNDCG@10: 0.0861\tHR@10: 0.5704\n",
      "INFO:\ttrainer:\tEpoch 106/500:\tLoss: 0.5109\tP@10: 0.0870\tR@10: 0.0435\tMAP@10: 0.0334\tNDCG@10: 0.0902\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 107/500:\tLoss: 0.5164\tP@10: 0.0914\tR@10: 0.0457\tMAP@10: 0.0339\tNDCG@10: 0.0918\tHR@10: 0.5898\n",
      "INFO:\ttrainer:\tEpoch 108/500:\tLoss: 0.5144\tP@10: 0.0901\tR@10: 0.0451\tMAP@10: 0.0322\tNDCG@10: 0.0882\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 109/500:\tLoss: 0.5109\tP@10: 0.0875\tR@10: 0.0438\tMAP@10: 0.0327\tNDCG@10: 0.0887\tHR@10: 0.5669\n",
      "INFO:\ttrainer:\tEpoch 110/500:\tLoss: 0.5086\tP@10: 0.0866\tR@10: 0.0433\tMAP@10: 0.0339\tNDCG@10: 0.0911\tHR@10: 0.5739\n",
      "INFO:\ttrainer:\tEpoch 111/500:\tLoss: 0.5162\tP@10: 0.0857\tR@10: 0.0429\tMAP@10: 0.0333\tNDCG@10: 0.0892\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 112/500:\tLoss: 0.5184\tP@10: 0.0819\tR@10: 0.0409\tMAP@10: 0.0340\tNDCG@10: 0.0890\tHR@10: 0.5511\n",
      "INFO:\ttrainer:\tEpoch 113/500:\tLoss: 0.5185\tP@10: 0.0833\tR@10: 0.0416\tMAP@10: 0.0323\tNDCG@10: 0.0871\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 114/500:\tLoss: 0.5105\tP@10: 0.0843\tR@10: 0.0422\tMAP@10: 0.0311\tNDCG@10: 0.0856\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 115/500:\tLoss: 0.5067\tP@10: 0.0868\tR@10: 0.0434\tMAP@10: 0.0318\tNDCG@10: 0.0870\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 116/500:\tLoss: 0.5085\tP@10: 0.0889\tR@10: 0.0445\tMAP@10: 0.0346\tNDCG@10: 0.0922\tHR@10: 0.5810\n",
      "INFO:\ttrainer:\tEpoch 117/500:\tLoss: 0.5097\tP@10: 0.0894\tR@10: 0.0447\tMAP@10: 0.0332\tNDCG@10: 0.0899\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 118/500:\tLoss: 0.5128\tP@10: 0.0879\tR@10: 0.0439\tMAP@10: 0.0336\tNDCG@10: 0.0906\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 119/500:\tLoss: 0.5133\tP@10: 0.0910\tR@10: 0.0455\tMAP@10: 0.0363\tNDCG@10: 0.0952\tHR@10: 0.5845\n",
      "INFO:\ttrainer:\tEpoch 120/500:\tLoss: 0.5158\tP@10: 0.0857\tR@10: 0.0429\tMAP@10: 0.0337\tNDCG@10: 0.0897\tHR@10: 0.5651\n",
      "INFO:\ttrainer:\tEpoch 121/500:\tLoss: 0.5154\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0330\tNDCG@10: 0.0898\tHR@10: 0.5687\n",
      "INFO:\ttrainer:\tEpoch 122/500:\tLoss: 0.5070\tP@10: 0.0850\tR@10: 0.0425\tMAP@10: 0.0333\tNDCG@10: 0.0889\tHR@10: 0.5528\n",
      "INFO:\ttrainer:\tEpoch 123/500:\tLoss: 0.5153\tP@10: 0.0896\tR@10: 0.0448\tMAP@10: 0.0346\tNDCG@10: 0.0927\tHR@10: 0.5775\n",
      "INFO:\ttrainer:\tEpoch 124/500:\tLoss: 0.5102\tP@10: 0.0833\tR@10: 0.0416\tMAP@10: 0.0321\tNDCG@10: 0.0863\tHR@10: 0.5599\n",
      "INFO:\ttrainer:\tEpoch 125/500:\tLoss: 0.5083\tP@10: 0.0886\tR@10: 0.0443\tMAP@10: 0.0345\tNDCG@10: 0.0919\tHR@10: 0.5722\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 125\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run10/model.pth\n",
      "INFO:\tmain:\tCompleted run 10\n",
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed_divide/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 3,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run11/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run11\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 3 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 2.6532\tP@10: 0.0658\tR@10: 0.0329\tMAP@10: 0.0273\tNDCG@10: 0.0738\tHR@10: 0.4894\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 3.2046\tP@10: 0.0408\tR@10: 0.0204\tMAP@10: 0.0155\tNDCG@10: 0.0442\tHR@10: 0.3169\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 3.5129\tP@10: 0.0428\tR@10: 0.0214\tMAP@10: 0.0102\tNDCG@10: 0.0365\tHR@10: 0.3504\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 3.2730\tP@10: 0.0264\tR@10: 0.0132\tMAP@10: 0.0071\tNDCG@10: 0.0248\tHR@10: 0.2377\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 3.3736\tP@10: 0.0533\tR@10: 0.0267\tMAP@10: 0.0172\tNDCG@10: 0.0523\tHR@10: 0.4190\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 3.3253\tP@10: 0.0544\tR@10: 0.0272\tMAP@10: 0.0177\tNDCG@10: 0.0538\tHR@10: 0.4243\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 3.2143\tP@10: 0.0461\tR@10: 0.0231\tMAP@10: 0.0153\tNDCG@10: 0.0467\tHR@10: 0.3803\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 3.2408\tP@10: 0.0577\tR@10: 0.0289\tMAP@10: 0.0179\tNDCG@10: 0.0559\tHR@10: 0.4630\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 3.0816\tP@10: 0.0674\tR@10: 0.0337\tMAP@10: 0.0211\tNDCG@10: 0.0648\tHR@10: 0.5053\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 2.9726\tP@10: 0.0651\tR@10: 0.0326\tMAP@10: 0.0244\tNDCG@10: 0.0693\tHR@10: 0.4947\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 2.8603\tP@10: 0.0595\tR@10: 0.0298\tMAP@10: 0.0247\tNDCG@10: 0.0674\tHR@10: 0.4701\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 11\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run11/model.pth\n",
      "INFO:\tmain:\tCompleted run 11\n",
      "{   'CAL_head_num': 2,\n",
      "    'FFN': 'PointWise',\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 128,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-100k',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed_divide/ml-100k/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 4,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': 'runs/run12/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run12\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-100k\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 568 \n",
      "INFO:\tdataloader:\tlocid2detail: 1439 \n",
      "INFO:\tdataloader:\tnode_num: 2012 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 568 \n",
      "INFO:\tdataloader:\ttrain_num: 4544 \n",
      "INFO:\tdataloader:\ttest_num: 568 \n",
      "INFO:\tdataloader:\ttrain_set: (4544, 13) \n",
      "INFO:\tdataloader:\ttest_set: (568, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 568 \n",
      "INFO:\tdataloader:\tu2v: 568 \n",
      "INFO:\tdataloader:\tu2vc: 568 \n",
      "INFO:\tdataloader:\tv2u: 1439 \n",
      "INFO:\tdataloader:\tv2vc: 1439 \n",
      "INFO:\tmain:\tCommencing training for 500 epochs. Please follow Training Logger from here on.\n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 4 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(1439, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(1439, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(2012, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\ttrainer:\tEpoch 1/500:\tLoss: 2.5093\tP@10: 0.0771\tR@10: 0.0386\tMAP@10: 0.0239\tNDCG@10: 0.0731\tHR@10: 0.5581\n",
      "INFO:\ttrainer:\tEpoch 2/500:\tLoss: 3.2370\tP@10: 0.0287\tR@10: 0.0143\tMAP@10: 0.0126\tNDCG@10: 0.0343\tHR@10: 0.2447\n",
      "INFO:\ttrainer:\tEpoch 3/500:\tLoss: 3.4388\tP@10: 0.0463\tR@10: 0.0232\tMAP@10: 0.0202\tNDCG@10: 0.0549\tHR@10: 0.3785\n",
      "INFO:\ttrainer:\tEpoch 4/500:\tLoss: 3.2085\tP@10: 0.0498\tR@10: 0.0249\tMAP@10: 0.0178\tNDCG@10: 0.0521\tHR@10: 0.4014\n",
      "INFO:\ttrainer:\tEpoch 5/500:\tLoss: 3.6012\tP@10: 0.0602\tR@10: 0.0301\tMAP@10: 0.0159\tNDCG@10: 0.0534\tHR@10: 0.4701\n",
      "INFO:\ttrainer:\tEpoch 6/500:\tLoss: 3.3624\tP@10: 0.0431\tR@10: 0.0216\tMAP@10: 0.0106\tNDCG@10: 0.0377\tHR@10: 0.3627\n",
      "INFO:\ttrainer:\tEpoch 7/500:\tLoss: 3.3469\tP@10: 0.0394\tR@10: 0.0197\tMAP@10: 0.0093\tNDCG@10: 0.0336\tHR@10: 0.3310\n",
      "INFO:\ttrainer:\tEpoch 8/500:\tLoss: 3.4175\tP@10: 0.0504\tR@10: 0.0252\tMAP@10: 0.0129\tNDCG@10: 0.0448\tHR@10: 0.4173\n",
      "INFO:\ttrainer:\tEpoch 9/500:\tLoss: 3.1516\tP@10: 0.0593\tR@10: 0.0297\tMAP@10: 0.0188\tNDCG@10: 0.0574\tHR@10: 0.4560\n",
      "INFO:\ttrainer:\tEpoch 10/500:\tLoss: 3.3299\tP@10: 0.0526\tR@10: 0.0263\tMAP@10: 0.0143\tNDCG@10: 0.0478\tHR@10: 0.4225\n",
      "INFO:\ttrainer:\tEpoch 11/500:\tLoss: 3.0215\tP@10: 0.0458\tR@10: 0.0229\tMAP@10: 0.0117\tNDCG@10: 0.0407\tHR@10: 0.3838\n",
      "INFO:\ttrainer:\tEarly stopping at epoch 11\n",
      "INFO:\tmain:\tCompleted training for 500 epochs\n",
      "INFO:\tmain:\tModel saved at runs/run12/model.pth\n",
      "INFO:\tmain:\tCompleted run 12\n"
     ]
    }
   ],
   "source": [
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_divide/ml-100k/ --model_variant 1\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_divide/ml-100k/ --model_variant 2\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_divide/ml-100k/ --model_variant 3\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_divide/ml-100k/ --model_variant 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movielens-1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-PCIE-32GB (UUID: GPU-6836cac4-6472-db93-c539-eaa93a70b15b)\n",
      "/home/FYP/siddhant005/.conda/envs/retagnn_pyg/bin/python\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L\n",
    "! which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'CAL_head_num': 2,\n",
      "    'H': 3,\n",
      "    'L': 10,\n",
      "    'TSAL_head_num': 2,\n",
      "    'attn_drop': 0.1,\n",
      "    'batch_size': 1024,\n",
      "    'block_backprop': False,\n",
      "    'conv_layer_num': 3,\n",
      "    'dataset': 'ml-1m',\n",
      "    'debug': False,\n",
      "    'device': device(type='cuda'),\n",
      "    'dim': 32,\n",
      "    'epoch_num': 500,\n",
      "    'file_path': '/home/FYP/siddhant005/fyp/code/data/processed/ml-1m/',\n",
      "    'hop': 2,\n",
      "    'l2': 0.0001,\n",
      "    'learning_rate': 0.001,\n",
      "    'model_variant': 1,\n",
      "    'negative_num': 2,\n",
      "    'num_bases': 3,\n",
      "    'optimizer': 'adam',\n",
      "    'plot_dir': '/home/FYP/siddhant005/fyp/runs/run13/plots',\n",
      "    'seed': 42,\n",
      "    'short_conv_layer_num': 3,\n",
      "    'topk': 20,\n",
      "    'verbose': True}\n",
      "INFO:\tmain:\tOutputs will be saved in: runs/run13\n",
      "INFO:\tmain:\tUsing seed: 42\n",
      "INFO:\tmain:\tUsing device: cuda\n",
      "INFO:\tmain:\tCommencing data collection for ml-1m\n",
      "INFO:\tdataloader:\tData preprocessing completed\n",
      "INFO:\tdataloader:\tuid2locid_time: 4297 \n",
      "INFO:\tdataloader:\tlocid2detail: 3112 \n",
      "INFO:\tdataloader:\tnode_num: 7414 \n",
      "INFO:\tdataloader:\trelation_num: 4 \n",
      "INFO:\tdataloader:\tuser_num: 4297 \n",
      "INFO:\tdataloader:\ttrain_num: 34376 \n",
      "INFO:\tdataloader:\ttest_num: 4297 \n",
      "INFO:\tdataloader:\ttrain_set: (34376, 13) \n",
      "INFO:\tdataloader:\ttest_set: (4297, 10) \n",
      "INFO:\tdataloader:\tuid_list_: 4297 \n",
      "INFO:\tdataloader:\tu2v: 4297 \n",
      "INFO:\tdataloader:\tu2vc: 4297 \n",
      "INFO:\tdataloader:\tv2u: 3112 \n",
      "INFO:\tdataloader:\tv2vc: 3112 \n",
      "INFO:\ttrainer_utils:\tModel:\tCAGSRec with 1 variant\n",
      "INFO:\ttrainer_utils:\tCAGSRec(\n",
      "  (long_term_gnn): LongTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (short_term_gnn): ShortTermGNN(\n",
      "    (conv_modulelist): ModuleList(\n",
      "      (0-2): 3 x ImprovedGNNunit(32, 32, num_relations=4)\n",
      "    )\n",
      "  )\n",
      "  (temporal_seq_attn_layer): TemporalSequentialAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_layer): CrossAttentionLayer_v2(\n",
      "    (drop_layer): Dropout(p=0.1, inplace=False)\n",
      "    (feedforward): Sequential(\n",
      "      (0): PointWiseFeedForward(\n",
      "        (conv1): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (conv2): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): LayerNorm((192,), eps=1e-08, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (predict_emb_w): Embedding(3112, 192, padding_idx=0)\n",
      "  (predict_emb_b): Embedding(3112, 1, padding_idx=0)\n",
      "  (node_embeddings): Embedding(7414, 32, padding_idx=0)\n",
      ")\n",
      "INFO:\ttrainer_utils:\tOptimizer:\tAdam with initial lr = 0.001, l2 = 0.0001\n",
      "INFO:\ttrainer_utils:\tLR Scheduler:\tLinearLR\n",
      "INFO:\tmain:\tCommencing training for 500 epochs.\n"
     ]
    }
   ],
   "source": [
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed/ml-1m/ --model_variant 1\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed/ml-1m/ --model_variant 2\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed/ml-1m/ --model_variant 3\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed/ml-1m/ --model_variant 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-1m/ --model_variant 1\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-1m/ --model_variant 2\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-1m/ --model_variant 3\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_subtract/ml-1m/ --model_variant 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_divide/ml-1m/ --model_variant 1\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_divide/ml-1m/ --model_variant 2\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_divide/ml-1m/ --model_variant 3\n",
    "! python code/src2/main.py --file_path /home/FYP/siddhant005/fyp/code/data/processed_divide/ml-1m/ --model_variant 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retagnn_pyg",
   "language": "python",
   "name": "retagnn_pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
